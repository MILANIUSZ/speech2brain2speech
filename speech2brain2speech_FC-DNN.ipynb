{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f572ea4c",
   "metadata": {},
   "source": [
    "# **FC-DNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ec5e1468",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mne\n",
    "import pandas as pd\n",
    "import mne_bids\n",
    "import matplotlib.pyplot as plt\n",
    "import WaveGlow_functions\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import EarlyStopping, CSVLogger, ModelCheckpoint\n",
    "import torch\n",
    "import datetime\n",
    "import gc\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import argparse\n",
    "from datetime import datetime, timedelta\n",
    "from subprocess import call, check_output, run\n",
    "import matplotlib.pyplot as plt\n",
    "import mne\n",
    "import numpy as np\n",
    "import scipy\n",
    "import librosa\n",
    "import scipy.signal\n",
    "import scipy.stats\n",
    "import scipy.io.wavfile\n",
    "import scipy.fftpack\n",
    "import scipy.io as sio\n",
    "import skimage.transform\n",
    "import soundfile as sf\n",
    "import tensorflow as tf\n",
    "import scipy\n",
    "import scipy.io\n",
    "# import scipy.io.wavfile\n",
    "import scipy.io.wavfile as io_wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bf6acbd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-19 21:32:55.567341: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:26:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-05-19 21:32:55.568175: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:26:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-05-19 21:32:55.568476: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:26:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-05-19 21:32:55.569527: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:26:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-05-19 21:32:55.569579: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-05-19 21:32:55.569858: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:26:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-05-19 21:32:55.569918: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5441 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070, pci bus id: 0000:26:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "# do not use all GPU memory\n",
    "from tensorflow.compat.v1.keras.backend import set_session\n",
    "import tensorflow.compat.v1 as tf\n",
    "config = tf.ConfigProto()\n",
    "# config.gpu_options.per_process_gpu_memory_fraction = 0.3\n",
    "config.gpu_options.allow_growth = True \n",
    "sess = tf.Session(config=config)\n",
    "set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ba85a7",
   "metadata": {},
   "source": [
    "### **Helper functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a1c9bd06",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "#Small helper function to speed up the hilbert transform by extending the length of data to the next power of 2\n",
    "hilbert3 = lambda x: scipy.signal.hilbert(x, scipy.fftpack.next_fast_len(len(x)),axis=0)[:len(x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1160075d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def extractHG(data, sr, windowLength=0.05, frameshift=0.01, bandpass_min=70, bandpass_max=170):\n",
    "    \"\"\"\n",
    "    Window data and extract frequency-band envelope using the hilbert transform\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data: array (samples, channels)\n",
    "        EEG time series\n",
    "    sr: int\n",
    "        Sampling rate of the data\n",
    "    windowLength: float\n",
    "        Length of window (in seconds) in which spectrogram will be calculated\n",
    "    frameshift: float\n",
    "        Shift (in seconds) after which next window will be extracted\n",
    "    Returns\n",
    "    ----------\n",
    "    feat: array (windows, channels)\n",
    "        Frequency-band feature matrix\n",
    "    \"\"\"\n",
    "    #Linear detrend\n",
    "    data = scipy.signal.detrend(data,axis=0)\n",
    "    #Number of windows\n",
    "    numWindows = int(np.floor((data.shape[0]-windowLength*sr)/(frameshift*sr)))\n",
    "    #Filter High-Gamma Band\n",
    "    # sos = scipy.signal.iirfilter(4, [70/(sr/2),170/(sr/2)],btype='bandpass',output='sos')\n",
    "    sos = scipy.signal.iirfilter(4, [bandpass_min/(sr/2),bandpass_max/(sr/2)],btype='bandpass',output='sos')\n",
    "    data = scipy.signal.sosfiltfilt(sos,data,axis=0)\n",
    "    #Attenuate first harmonic of line noise\n",
    "    # sos = scipy.signal.iirfilter(4, [98/(sr/2),102/(sr/2)],btype='bandstop',output='sos')\n",
    "    # data = scipy.signal.sosfiltfilt(sos,data,axis=0)\n",
    "    #Attenuate second harmonic of line noise\n",
    "    # sos = scipy.signal.iirfilter(4, [148/(sr/2),152/(sr/2)],btype='bandstop',output='sos')\n",
    "    # data = scipy.signal.sosfiltfilt(sos,data,axis=0)\n",
    "    #Create feature space\n",
    "    data = np.abs(hilbert3(data))\n",
    "    feat = np.zeros((numWindows,data.shape[1]))\n",
    "    for win in range(numWindows):\n",
    "        start= int(np.floor((win*frameshift)*sr))\n",
    "        stop = int(np.floor(start+windowLength*sr))\n",
    "        feat[win,:] = np.mean(data[start:stop,:],axis=0)\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5740dbe7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def stackFeatures(features, modelOrder=4, stepSize=5):\n",
    "    \"\"\"\n",
    "    Add temporal context to each window by stacking neighboring feature vectors\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    features: array (windows, channels)\n",
    "        Feature time series\n",
    "    modelOrder: int\n",
    "        Number of temporal context to include prior to and after current window\n",
    "    stepSize: float\n",
    "        Number of temporal context to skip for each next context (to compensate for frameshift)\n",
    "    Returns\n",
    "    ----------\n",
    "    featStacked: array (windows, feat*(2*modelOrder+1))\n",
    "        Stacked feature matrix\n",
    "    \"\"\"\n",
    "    featStacked=np.zeros((features.shape[0]-(2*modelOrder*stepSize),(2*modelOrder+1)*features.shape[1]))\n",
    "    for fNum,i in enumerate(range(modelOrder*stepSize,features.shape[0]-modelOrder*stepSize)):\n",
    "        ef=features[i-modelOrder*stepSize:i+modelOrder*stepSize+1:stepSize,:]\n",
    "        featStacked[fNum,:]=ef.flatten() #Add 'F' if stacked the same as matlab\n",
    "    return featStacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7ec2377f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WaveGlow / Tacotron2 / STFT parameters for audio data\n",
    "# samplingFrequency = 16000\n",
    "samplingFrequency = 22050\n",
    "#samplingFrequency_EEG = 512 #sub 07\n",
    "winL_EEG = 0.05\n",
    "# frameshift_EEG = 0.01 # 10 ms\n",
    "frameshift_EEG = 0.01 # 10 ms\n",
    "frameshift_speech = 220 # 10ms\n",
    "# modelOrder_EEG = 1\n",
    "# modelOrder_EEG = 2\n",
    "modelOrder_EEG = 4\n",
    "# modelOrder_EEG = 10\n",
    "stepSize_EEG = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "dac943e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/environment/notebooks/speech2brain2speech/WaveGlow_functions.py:136: FutureWarning: Pass size=1024 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  fft_window = pad_center(fft_window, filter_length)\n",
      "/environment/notebooks/speech2brain2speech/WaveGlow_functions.py:222: FutureWarning: Pass sr=22050, n_fft=1024, n_mels=80, fmin=0, fmax=8000 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  mel_basis = librosa_mel_fn(\n"
     ]
    }
   ],
   "source": [
    "stft = WaveGlow_functions.TacotronSTFT(\n",
    "        filter_length=1024,\n",
    "        hop_length=frameshift_speech,\n",
    "        win_length=1024,\n",
    "        n_mel_channels=80,\n",
    "        sampling_rate=samplingFrequency,\n",
    "        mel_fmin=0,\n",
    "        mel_fmax=8000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ef706b",
   "metadata": {},
   "source": [
    "### **Load Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9a83068a",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#Load Audio\u001b[39;00m\n\u001b[1;32m     17\u001b[0m wavfile \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshifted_audio.wav\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 18\u001b[0m mel_data \u001b[38;5;241m=\u001b[39m \u001b[43mWaveGlow_functions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_mel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwavfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstft\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m mel_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfliplr(np\u001b[38;5;241m.\u001b[39mrot90(mel_data\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mnumpy(), axes\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m)))\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Print out the duration of the original and shifted audio\u001b[39;00m\n",
      "File \u001b[0;32m/environment/notebooks/speech2brain2speech/WaveGlow_functions.py:261\u001b[0m, in \u001b[0;36mget_mel\u001b[0;34m(filename, stft)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_mel\u001b[39m(filename,stft):\n\u001b[0;32m--> 261\u001b[0m             audio, sampling_rate \u001b[38;5;241m=\u001b[39m \u001b[43mload_wav_to_torch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    262\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m sampling_rate \u001b[38;5;241m!=\u001b[39m stft\u001b[38;5;241m.\u001b[39msampling_rate:\n\u001b[1;32m    263\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m SR doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt match target \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m SR\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    264\u001b[0m                     sampling_rate, stft\u001b[38;5;241m.\u001b[39msampling_rate))\n",
      "File \u001b[0;32m/environment/notebooks/speech2brain2speech/WaveGlow_functions.py:256\u001b[0m, in \u001b[0;36mload_wav_to_torch\u001b[0;34m(full_path)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_wav_to_torch\u001b[39m(full_path):\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;66;03m#sampling_rate, data = read(full_path)\u001b[39;00m\n\u001b[0;32m--> 256\u001b[0m     data,sampling_rate \u001b[38;5;241m=\u001b[39m \u001b[43mlibrosa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43msr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m22050\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mmono\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;66;03m# data,sampling_rate = librosa.load(full_path,sr=16000,mono=True)\u001b[39;00m\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mFloatTensor(data\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)), sampling_rate\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/librosa/util/decorators.py:88\u001b[0m, in \u001b[0;36mdeprecate_positional_args.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m extra_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(all_args)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m extra_args \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# extra_args > 0\u001b[39;00m\n\u001b[1;32m     91\u001b[0m args_msg \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(name, arg)\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(kwonly_args[:extra_args], args[\u001b[38;5;241m-\u001b[39mextra_args:])\n\u001b[1;32m     94\u001b[0m ]\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/librosa/core/audio.py:179\u001b[0m, in \u001b[0;36mload\u001b[0;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[1;32m    176\u001b[0m     y \u001b[38;5;241m=\u001b[39m to_mono(y)\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mresample\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morig_sr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msr_native\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_sr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mres_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mres_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    182\u001b[0m     sr \u001b[38;5;241m=\u001b[39m sr_native\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/librosa/util/decorators.py:88\u001b[0m, in \u001b[0;36mdeprecate_positional_args.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m extra_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(all_args)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m extra_args \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# extra_args > 0\u001b[39;00m\n\u001b[1;32m     91\u001b[0m args_msg \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(name, arg)\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(kwonly_args[:extra_args], args[\u001b[38;5;241m-\u001b[39mextra_args:])\n\u001b[1;32m     94\u001b[0m ]\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/librosa/core/audio.py:647\u001b[0m, in \u001b[0;36mresample\u001b[0;34m(y, orig_sr, target_sr, res_type, fix, scale, **kwargs)\u001b[0m\n\u001b[1;32m    645\u001b[0m     y_hat \u001b[38;5;241m=\u001b[39m soxr\u001b[38;5;241m.\u001b[39mresample(y\u001b[38;5;241m.\u001b[39mT, orig_sr, target_sr, quality\u001b[38;5;241m=\u001b[39mres_type)\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m    646\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 647\u001b[0m     y_hat \u001b[38;5;241m=\u001b[39m \u001b[43mresampy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresample\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morig_sr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_sr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mres_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fix:\n\u001b[1;32m    650\u001b[0m     y_hat \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mfix_length(y_hat, size\u001b[38;5;241m=\u001b[39mn_samples, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/resampy/core.py:168\u001b[0m, in \u001b[0;36mresample\u001b[0;34m(x, sr_orig, sr_new, axis, filter, parallel, **kwargs)\u001b[0m\n\u001b[1;32m    158\u001b[0m         resample_f_s(\n\u001b[1;32m    159\u001b[0m             x\u001b[38;5;241m.\u001b[39mswapaxes(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, axis),\n\u001b[1;32m    160\u001b[0m             t_out,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    165\u001b[0m             y\u001b[38;5;241m.\u001b[39mswapaxes(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, axis),\n\u001b[1;32m    166\u001b[0m         )\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 168\u001b[0m     \u001b[43mresample_f_s\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mswapaxes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_out\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43minterp_win\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43minterp_delta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprecision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mswapaxes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/numba/np/ufunc/gufunc.py:192\u001b[0m, in \u001b[0;36mGUFunc.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd(sig)\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_ufunc()\n\u001b[0;32m--> 192\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mufunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pydub import AudioSegment\n",
    "\n",
    "# Load Audio\n",
    "wavfile = 'data/stimuli/6min.wav'\n",
    "audio = AudioSegment.from_file(wavfile, format='wav')\n",
    "\n",
    "# Shift audio by 150 ms\n",
    "shifted_audio = audio._spawn(audio.raw_data, overrides={'frame_rate': audio.frame_rate, 'frame_width': audio.sample_width})\n",
    "shifted_audio = shifted_audio._spawn(shifted_audio.raw_data, overrides={'frame_rate': shifted_audio.frame_rate + int(22050*0.15)})\n",
    "shifted_audio = shifted_audio.set_frame_rate(audio.frame_rate)\n",
    "shifted_audio = shifted_audio.set_channels(audio.channels)\n",
    "\n",
    "# Export shifted audio as WAV file\n",
    "shifted_audio.export('shifted_audio.wav', format='wav')\n",
    "\n",
    "#Load Audio\n",
    "wavfile = 'shifted_audio.wav'\n",
    "mel_data = WaveGlow_functions.get_mel(wavfile, stft)\n",
    "mel_data = np.fliplr(np.rot90(mel_data.data.numpy(), axes=(1, 0)))\n",
    "\n",
    "\n",
    "# Print out the duration of the original and shifted audio\n",
    "print(\"Original audio duration:\", audio.duration_seconds)\n",
    "print(\"Shifted audio duration:\", shifted_audio.duration_seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6866e010",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(mel_data, aspect='auto')\n",
    "plt.xlabel('Frame')\n",
    "plt.ylabel('Mel Bin')\n",
    "plt.title('Mel Spectrogram of the Audio')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa956309",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.transpose(mel_data), aspect='auto')\n",
    "plt.xlabel('Mel Bin')\n",
    "plt.ylabel('Frame')\n",
    "plt.title('Transposed Mel Spectrogram of the Audio')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2be4410",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Subjects\n",
    "bids_dir = 'data'\n",
    "subjects = mne_bids.get_entity_vals(bids_dir, 'subject')\n",
    "#Choose subjects\n",
    "subject = '46'\n",
    "acquisition = 'clinical'\n",
    "task = 'film'\n",
    "datatype = 'ieeg'\n",
    "session = 'iemu'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3022aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load subject's channels\n",
    "channels_path = mne_bids.BIDSPath(subject=subject,\n",
    "                                    session=session,\n",
    "                                    suffix='channels',\n",
    "                                    extension='tsv',\n",
    "                                    datatype=datatype,\n",
    "                                    task=task,\n",
    "                                    acquisition=acquisition,\n",
    "                                    root=bids_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989d5a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = pd.read_csv(str(channels_path.match()[0]), sep='\\t', header=0, index_col=None)\n",
    "#print(channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e2b3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set channel types\n",
    "data_path = mne_bids.BIDSPath(subject=subject,\n",
    "                                    session=session,\n",
    "                                    suffix='ieeg',\n",
    "                                    extension='vhdr',\n",
    "                                    datatype=datatype,\n",
    "                                    task=task,\n",
    "                                    acquisition=acquisition,\n",
    "                                    root=bids_dir)\n",
    "raw = mne.io.read_raw_brainvision(str(data_path.match()[0]), scale=1.0, preload=False, verbose=True)\n",
    "raw.set_channel_types({ch_name: str(x).lower()\n",
    "                if str(x).lower() in ['ecog', 'seeg', 'eeg'] else 'misc'\n",
    "                                for ch_name, x in zip(raw.ch_names, channels['type'].values)})\n",
    "raw.drop_channels([raw.ch_names[i] for i, j in enumerate(raw.get_channel_types()) if j == 'misc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a36615",
   "metadata": {},
   "source": [
    "print(raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501706bb",
   "metadata": {},
   "source": [
    "### Discard Bad Channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83882f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bad channels\n",
    "bad_channels = channels['name'][(channels['type'].isin(['ECOG', 'SEEG'])) & (channels['status'] == 'bad')].tolist()\n",
    "raw.info['bads'].extend([ch for ch in bad_channels])\n",
    "raw.drop_channels(raw.info['bads'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ad223c",
   "metadata": {},
   "source": [
    "### Load Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a51320d",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae79701",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_eeg_channels = int(raw.info['nchan']) # for subject 01\n",
    "print('n_eeg_channels', n_eeg_channels)\n",
    "# raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e11f21",
   "metadata": {},
   "source": [
    "### Apply notch filter to remove line noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a7e45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw.notch_filter(freqs=np.arange(50, 251, 50))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c1a751",
   "metadata": {},
   "source": [
    "raw.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1a92fe",
   "metadata": {},
   "source": [
    "### Apply common average reference to remove common noise and trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be08d03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CAR\n",
    "raw_car, _ = mne.set_eeg_reference(raw.copy(), 'average')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4bfcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = raw_car.copy().filter(60, 120).apply_hilbert(envelope=True).get_data()#.T\n",
    "print('raw_car.shape:', raw_car._data.shape, 'gamma shape: ', gamma.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad375f65",
   "metadata": {},
   "source": [
    "#Extract signal in gamma range, use Hilbert transform, but can also play around with wavelet decomposition options\n",
    "\n",
    "\n",
    "gamma = raw_car.copy().filter(60, 120).apply_hilbert(envelope=True).get_data().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b220ef9",
   "metadata": {},
   "source": [
    "### Read annotation with event markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c0237f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "custom_mapping = {'Stimulus/music': 2, 'Stimulus/speech': 1,\n",
    "                  'Stimulus/end task': 5}  # 'Stimulus/task end' in laan\n",
    "events, event_id = mne.events_from_annotations(raw_car, event_id=custom_mapping,\n",
    "                                                         use_rounding=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beda567e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting\n",
    "raw_car.plot(n_channels=30,scalings='auto', duration=3, start=29)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d88b433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume gamma is a 2D array\n",
    "np.savetxt(\"gamma.tsv\", gamma, delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7fa713",
   "metadata": {},
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a5638f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get EEG SR\n",
    "n_melspec = 80\n",
    "samplingFrequency_EEG=raw_car.info['sfreq']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58f7173",
   "metadata": {},
   "source": [
    "### Crop to keep only the segments while wathcing the stimuli ( 6.5 min long movie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f7bacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a copy taht we crop\n",
    "raw_car_cut = raw_car._data.copy()\n",
    "print(raw_car_cut.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c8a262",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('before cut: ', raw_car._data.shape, mel_data.shape)\n",
    "\n",
    "raw_car_cut = np.empty((n_eeg_channels,0))\n",
    "mel_data_cut = np.empty((0,n_melspec))\n",
    "\n",
    "\n",
    "# for i in range(6):\n",
    "for i in range(6):\n",
    "    start_time = events[2*i+1, 0] / raw_car.info['sfreq']\n",
    "    end_time = events[2*i+2, 0] / raw_car.info['sfreq']\n",
    "    start_idx, end_idx = raw_car.time_as_index([start_time, end_time])\n",
    "    print(i, 'iEEG index', start_idx, end_idx, end_idx-start_idx)\n",
    "    n_frames_per_sec = int(1 / frameshift_EEG)\n",
    "    print(i, 'melspec index', (2*i+1)*30*n_frames_per_sec, (2*i+2)*30*n_frames_per_sec, (2*i+2)*30*n_frames_per_sec-(2*i+1)*30*n_frames_per_sec)\n",
    "    # raw_car_cut1 = raw_car._data[:, start_idx:end_idx]\n",
    "    raw_car_cut1 = gamma[:, start_idx:end_idx]\n",
    "    raw_car_cut = np.append(raw_car_cut, raw_car_cut1, axis=1)\n",
    "    mel_data_cut1 = mel_data[(2*i+1)*30*n_frames_per_sec : (2*i+2)*30*n_frames_per_sec]\n",
    "    mel_data_cut = np.append(mel_data_cut, mel_data_cut1, axis=0)\n",
    "# raise\n",
    "mel_data = mel_data_cut\n",
    "\n",
    "print('after cut: ', raw_car_cut.shape, mel_data.shape)\n",
    "# raise\n",
    "#praat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b5b054",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get EEG SR\n",
    "samplingFrequency_EEG=raw_car.info['sfreq']\n",
    "\n",
    "# Calculate the length of the signal\n",
    "length = raw_car_cut.shape[1] / samplingFrequency_EEG \n",
    "\n",
    "print(\"The length of the EEG signal is\", length,\"s\")\n",
    "print(samplingFrequency_EEG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a49b6d",
   "metadata": {},
   "source": [
    "### Prepare Data for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c8a1d3",
   "metadata": {},
   "source": [
    "### Extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60849991",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract HG features\n",
    "print('calculating Hilbert...', raw_car_cut.shape)\n",
    "# eeg_fft = np.empty((n_max_frames, n_freq_bands, n_eeg_channels * (2 * modelOrder_EEG + 1) ))\n",
    "# feat_Hilbert_1 = extractHG(raw_car_cut,samplingFrequency_EEG, windowLength=winL_EEG,frameshift=frameshift_EEG, bandpass_min=1, bandpass_max=200)\n",
    "feat_Hilbert_1 = extractHG(np.rot90(raw_car_cut),samplingFrequency_EEG, windowLength=winL_EEG,frameshift=frameshift_EEG, bandpass_min=1, bandpass_max=200)\n",
    "# feat_Hilbert_2 = extractHG(np.rot90(current_raw_eeg_data),samplingFrequency_EEG, windowLength=winL_EEG,frameshift=frameshift_EEG, bandpass_min=51, bandpass_max=100)\n",
    "# feat_Hilbert_3 = extractHG(np.rot90(current_raw_eeg_data),samplingFrequency_EEG, windowLength=winL_EEG,frameshift=frameshift_EEG, bandpass_min=101, bandpass_max=150)\n",
    "# feat_Hilbert_4 = extractHG(np.rot90(current_raw_eeg_data),samplingFrequency_EEG, windowLength=winL_EEG,frameshift=frameshift_EEG, bandpass_min=151, bandpass_max=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d54791b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stack features\n",
    "feat_Hilbert_1 = stackFeatures(feat_Hilbert_1,modelOrder=modelOrder_EEG,stepSize=stepSize_EEG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f189341e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.subplot(211)\n",
    "plt.imshow(np.rot90(feat_Hilbert_1), aspect='auto')\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.imshow(np.rot90(mel_data), aspect='auto')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "eeg = feat_Hilbert_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd726bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_len = np.min((len(eeg), len(mel_data)))\n",
    "eeg = eeg[0:min_len]\n",
    "mel_data = mel_data[0:min_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b80c971",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('mel & iEEG: ', mel_data.shape, feat_Hilbert_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231022c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index = np.arange(0, int(0.8 * eeg.shape[0]))\n",
    "test_index = np.arange(int(0.8 * eeg.shape[0]), eeg.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f977bb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-validation-test split\n",
    "eeg_train = eeg[0 : int(len(eeg) * 0.8)]\n",
    "eeg_valid = eeg[int(len(eeg) * 0.8) : int(len(eeg) * 0.9)]\n",
    "eeg_test =  eeg[int(len(eeg) * 0.9) : ]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95580bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "melspec_train = mel_data[0 : int(len(mel_data) * 0.8)]\n",
    "melspec_valid = mel_data[int(len(mel_data) * 0.8) : int(len(mel_data) * 0.9)]\n",
    "melspec_test =  mel_data[int(len(mel_data) * 0.9) : ]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45cbe30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale input to [0-1]\n",
    "eeg_scaler = MinMaxScaler()\n",
    "# eeg_scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "eeg_train_scaled = eeg_scaler.fit_transform(eeg_train)\n",
    "eeg_valid_scaled = eeg_scaler.transform(eeg_valid)\n",
    "eeg_test_scaled  = eeg_scaler.transform(eeg_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ea75cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale outpit mel-spectrogram data to zero mean, unit variances\n",
    "melspec_scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "melspec_train_scaled = melspec_scaler.fit_transform(melspec_train)\n",
    "melspec_valid_scaled = melspec_scaler.transform(melspec_valid)\n",
    "melspec_test_scaled  = melspec_scaler.transform(melspec_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c2cc3f",
   "metadata": {},
   "source": [
    "### FC-DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb69759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 hidden layers, with 1000 neuron on each layer\n",
    "model = Sequential()\n",
    "model.add(\n",
    "    Dense(\n",
    "       8000,\n",
    "        input_dim=eeg_train_scaled.shape[1],\n",
    "        kernel_initializer='normal',\n",
    "        activation='relu'))\n",
    "\n",
    "model.add(\n",
    "    Dense(\n",
    "       4000,\n",
    "        input_dim=eeg_train_scaled.shape[1],\n",
    "        kernel_initializer='normal',\n",
    "        activation='relu'))\n",
    "model.add(\n",
    "    Dense(\n",
    "        80,\n",
    "        kernel_initializer='normal',\n",
    "        activation='linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2453c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model\n",
    "model.compile(\n",
    "    loss='mean_squared_error',\n",
    "    metrics=['mean_squared_error'],\n",
    "    optimizer='adam')\n",
    "earlystopper = EarlyStopping(\n",
    "    monitor='val_mean_squared_error',\n",
    "    min_delta=0.001,\n",
    "    patience=5,\n",
    "    verbose=1,\n",
    "    mode='auto')\n",
    "\n",
    "if not (os.path.isdir('models_iEEG_to_melspec/')):\n",
    "    os.mkdir('models_iEEG_to_melspec/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec442183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# early stopping to avoid over-training\n",
    "# csv logger\n",
    "current_date = '{date:%Y-%m-%d_%H-%M-%S}'.format(\n",
    "    date=datetime.now())\n",
    "print(current_date)\n",
    "# n_eeg_channels * (2 * modelOrder_EEG + 1)\n",
    "model_name = 'models_iEEG_to_melspec/iEEG-Hilbert_to_melspec_DNN_modelOrder-' + str(modelOrder_EEG).zfill(2) + '_freqBands-1_' + '_sub' + subject + '_' + current_date\n",
    "logger = CSVLogger(model_name + '.csv', append=True, separator=';')\n",
    "checkp = ModelCheckpoint(\n",
    "    model_name +\n",
    "    '_weights_best.h5',\n",
    "    monitor='val_loss',\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    mode='min')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea32df2",
   "metadata": {},
   "source": [
    "serialize scalers to pickle\n",
    "pickle.dump(eeg_scaler, open(model_name + '_eeg_scaler.sav', 'wb'))\n",
    "pickle.dump(melspec_scaler, open(model_name + '_melspec_scaler.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685666f3",
   "metadata": {},
   "source": [
    "### Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f957197",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(eeg_train_scaled, melspec_train_scaled,\n",
    "                    epochs=50, batch_size=64, shuffle=True, verbose=1,\n",
    "                    callbacks=[earlystopper, logger, checkp],\n",
    "                    validation_split = 0.9,\n",
    "                    validation_data=(eeg_valid_scaled, melspec_valid_scaled),\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925c663e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "model_json = model.to_json()\n",
    "with open(model_name + '_model.json', \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "    \n",
    "# here the training of the DNN is finished\n",
    "# load back best weights\n",
    "model.load_weights(model_name + '_weights_best.h5')\n",
    "# remove model file\n",
    "# os.remove(model_name + '_weights_best.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b48ac7",
   "metadata": {},
   "source": [
    "### Visualize predicted melspectogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01e70a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# melspec_predicted = model.predict(eeg_test_scaled[0:500])\n",
    "melspec_predicted = model.predict(eeg_test_scaled[0:1000])\n",
    "# melspec_predicted = melspec_predicted[0:500]\n",
    "# test_melspec = test_melspec[]\n",
    "\n",
    "\n",
    "#best stuff\n",
    "\n",
    "best_val_mse = min(history.history['val_mean_squared_error'])\n",
    "print(f'Best validation MSE: {best_val_mse:.4f}')\n",
    "\n",
    "min_train_loss = min(history.history['loss'])\n",
    "print(f'Minimum training loss: {min_train_loss:.4f}')\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(3, 1, figsize=(10, 10))\n",
    "axs[0].imshow(np.rot90(eeg_test_scaled[0:1000]), aspect='auto')\n",
    "axs[0].set_title('EEG Test Scaled')\n",
    "axs[1].imshow(np.rot90(melspec_test_scaled[0:1000]), aspect='auto')\n",
    "axs[1].set_title('Mel-Spectrogram Test Scaled')\n",
    "axs[2].imshow(np.rot90(melspec_predicted[0:1000]), aspect='auto')\n",
    "axs[2].set_title('Mel-Spectrogram Predicted')\n",
    "plt.suptitle(f'FC-DNN results (scaled) for subject {subject}\\nBest validation MSE: {best_val_mse:.4f}, Minimum training loss: {min_train_loss:.4f}')\n",
    "plt.subplots_adjust(hspace=0.4)\n",
    "\n",
    "\n",
    "plt.savefig(model_name + '_EEG_scaled_plots.png')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9039e1e3",
   "metadata": {},
   "source": [
    "### Audio synth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a4107f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Provide the actual mel spectrogram (melspec_predicted) and sampling frequency (samplingFrequency) from your FC-DNN output\n",
    "samplingFrequency = 512\n",
    "\n",
    "# Convert mel spectrogram to audio using Griffin-Lim algorithm\n",
    "audio = librosa.feature.inverse.mel_to_audio(melspec_predicted, sr=samplingFrequency)\n",
    "\n",
    "# Save the audio to a WAV file\n",
    "output_file = 'predicted_audio_fc-DNN.wav'\n",
    "sf.write(output_file, audio, samplingFrequency)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
