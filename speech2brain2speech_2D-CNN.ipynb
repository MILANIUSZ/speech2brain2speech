{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f572ea4c",
   "metadata": {},
   "source": [
    "# **2D-CNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec5e1468",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmne\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmne_bids\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/__init__.py:141\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m testing  \u001b[38;5;66;03m# noqa:PDF015\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_print_versions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m show_versions\n\u001b[0;32m--> 141\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;66;03m# excel\u001b[39;00m\n\u001b[1;32m    143\u001b[0m     ExcelFile,\n\u001b[1;32m    144\u001b[0m     ExcelWriter,\n\u001b[1;32m    145\u001b[0m     read_excel,\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;66;03m# parsers\u001b[39;00m\n\u001b[1;32m    147\u001b[0m     read_csv,\n\u001b[1;32m    148\u001b[0m     read_fwf,\n\u001b[1;32m    149\u001b[0m     read_table,\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;66;03m# pickle\u001b[39;00m\n\u001b[1;32m    151\u001b[0m     read_pickle,\n\u001b[1;32m    152\u001b[0m     to_pickle,\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;66;03m# pytables\u001b[39;00m\n\u001b[1;32m    154\u001b[0m     HDFStore,\n\u001b[1;32m    155\u001b[0m     read_hdf,\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;66;03m# sql\u001b[39;00m\n\u001b[1;32m    157\u001b[0m     read_sql,\n\u001b[1;32m    158\u001b[0m     read_sql_query,\n\u001b[1;32m    159\u001b[0m     read_sql_table,\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;66;03m# misc\u001b[39;00m\n\u001b[1;32m    161\u001b[0m     read_clipboard,\n\u001b[1;32m    162\u001b[0m     read_parquet,\n\u001b[1;32m    163\u001b[0m     read_orc,\n\u001b[1;32m    164\u001b[0m     read_feather,\n\u001b[1;32m    165\u001b[0m     read_gbq,\n\u001b[1;32m    166\u001b[0m     read_html,\n\u001b[1;32m    167\u001b[0m     read_xml,\n\u001b[1;32m    168\u001b[0m     read_json,\n\u001b[1;32m    169\u001b[0m     read_stata,\n\u001b[1;32m    170\u001b[0m     read_sas,\n\u001b[1;32m    171\u001b[0m     read_spss,\n\u001b[1;32m    172\u001b[0m )\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjson\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _json_normalize \u001b[38;5;28;01mas\u001b[39;00m json_normalize\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_tester\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m test\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/api.py:37\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspss\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m read_spss\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     33\u001b[0m     read_sql,\n\u001b[1;32m     34\u001b[0m     read_sql_query,\n\u001b[1;32m     35\u001b[0m     read_sql_table,\n\u001b[1;32m     36\u001b[0m )\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m read_stata\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mxml\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m read_xml\n\u001b[1;32m     40\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExcelFile\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExcelWriter\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_pickle\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     65\u001b[0m ]\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:991\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:975\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:671\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:844\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:939\u001b[0m, in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1038\u001b[0m, in \u001b[0;36mget_data\u001b[0;34m(self, path)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import mne\n",
    "import pandas as pd\n",
    "import mne_bids\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import MelFilterBank as mel\n",
    "import reconstructWave as rW\n",
    "import tensorflow\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, InputLayer, Dropout\n",
    "from keras import regularizers\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import torch\n",
    "import sys\n",
    "import soundfile as sf\n",
    "import skimage.transform\n",
    "from tensorflow import keras\n",
    "import gc\n",
    "import WaveGlow_functions\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import datetime\n",
    "from keras.callbacks import CSVLogger\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import argparse\n",
    "from subprocess import call, check_output, run\n",
    "import scipy.stats\n",
    "import scipy.io.wavfile\n",
    "import scipy.fftpack\n",
    "import scipy.io as sio\n",
    "import scipy\n",
    "import librosa\n",
    "import librosa.display\n",
    "import tensorflow as tf\n",
    "\n",
    "# Additional imports\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import mne\n",
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "import soundfile as sf\n",
    "\n",
    "# Set TensorFlow GPU memory growth\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178db228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not use all GPU memory\n",
    "from tensorflow.compat.v1.keras.backend import set_session\n",
    "import tensorflow.compat.v1 as tf\n",
    "config = tf.ConfigProto()\n",
    "# config.gpu_options.per_process_gpu_memory_fraction = 0.3\n",
    "config.gpu_options.allow_growth = True \n",
    "sess = tf.Session(config=config)\n",
    "set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ba85a7",
   "metadata": {},
   "source": [
    "### **Helper functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c9bd06",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "#Small helper function to speed up the hilbert transform by extending the length of data to the next power of 2\n",
    "hilbert3 = lambda x: scipy.signal.hilbert(x, scipy.fftpack.next_fast_len(len(x)),axis=0)[:len(x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1160075d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def extractHG(data, sr, windowLength=0.05, frameshift=0.01, bandpass_min=70, bandpass_max=170):\n",
    "    \"\"\"\n",
    "    Window data and extract frequency-band envelope using the hilbert transform\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data: array (samples, channels)\n",
    "        EEG time series\n",
    "    sr: int\n",
    "        Sampling rate of the data\n",
    "    windowLength: float\n",
    "        Length of window (in seconds) in which spectrogram will be calculated\n",
    "    frameshift: float\n",
    "        Shift (in seconds) after which next window will be extracted\n",
    "    Returns\n",
    "    ----------\n",
    "    feat: array (windows, channels)\n",
    "        Frequency-band feature matrix\n",
    "    \"\"\"\n",
    "    #Linear detrend\n",
    "    data = scipy.signal.detrend(data,axis=0)\n",
    "    #Number of windows\n",
    "    numWindows = int(np.floor((data.shape[0]-windowLength*sr)/(frameshift*sr)))\n",
    "    #Filter High-Gamma Band\n",
    "    # sos = scipy.signal.iirfilter(4, [70/(sr/2),170/(sr/2)],btype='bandpass',output='sos')\n",
    "    sos = scipy.signal.iirfilter(4, [bandpass_min/(sr/2),bandpass_max/(sr/2)],btype='bandpass',output='sos')\n",
    "    data = scipy.signal.sosfiltfilt(sos,data,axis=0)\n",
    "    #Attenuate first harmonic of line noise\n",
    "    # sos = scipy.signal.iirfilter(4, [98/(sr/2),102/(sr/2)],btype='bandstop',output='sos')\n",
    "    # data = scipy.signal.sosfiltfilt(sos,data,axis=0)\n",
    "    #Attenuate second harmonic of line noise\n",
    "    # sos = scipy.signal.iirfilter(4, [148/(sr/2),152/(sr/2)],btype='bandstop',output='sos')\n",
    "    # data = scipy.signal.sosfiltfilt(sos,data,axis=0)\n",
    "    #Create feature space\n",
    "    data = np.abs(hilbert3(data))\n",
    "    feat = np.zeros((numWindows,data.shape[1]))\n",
    "    for win in range(numWindows):\n",
    "        start= int(np.floor((win*frameshift)*sr))\n",
    "        stop = int(np.floor(start+windowLength*sr))\n",
    "        feat[win,:] = np.mean(data[start:stop,:],axis=0)\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5740dbe7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def stackFeatures(features, modelOrder=4, stepSize=5):\n",
    "    \"\"\"\n",
    "    Add temporal context to each window by stacking neighboring feature vectors\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    features: array (windows, channels)\n",
    "        Feature time series\n",
    "    modelOrder: int\n",
    "        Number of temporal context to include prior to and after current window\n",
    "    stepSize: float\n",
    "        Number of temporal context to skip for each next context (to compensate for frameshift)\n",
    "    Returns\n",
    "    ----------\n",
    "    featStacked: array (windows, feat*(2*modelOrder+1))\n",
    "        Stacked feature matrix\n",
    "    \"\"\"\n",
    "    featStacked=np.zeros((features.shape[0]-(2*modelOrder*stepSize),(2*modelOrder+1)*features.shape[1]))\n",
    "    for fNum,i in enumerate(range(modelOrder*stepSize,features.shape[0]-modelOrder*stepSize)):\n",
    "        ef=features[i-modelOrder*stepSize:i+modelOrder*stepSize+1:stepSize,:]\n",
    "        featStacked[fNum,:]=ef.flatten() #Add 'F' if stacked the same as matlab\n",
    "    return featStacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec2377f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WaveGlow / Tacotron2 / STFT parameters for audio data\n",
    "# samplingFrequency = 16000\n",
    "samplingFrequency = 22050\n",
    "#samplingFrequency_EEG = 512 #sub 07\n",
    "winL_EEG = 0.05\n",
    "# frameshift_EEG = 0.01 # 10 ms\n",
    "frameshift_EEG = 0.01 # 10 ms\n",
    "frameshift_speech = 220 # 10ms\n",
    "# modelOrder_EEG = 1\n",
    "# modelOrder_EEG = 2\n",
    "modelOrder_EEG = 4\n",
    "# modelOrder_EEG = 10\n",
    "stepSize_EEG = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac943e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "stft = WaveGlow_functions.TacotronSTFT(\n",
    "        filter_length=1024,\n",
    "        hop_length=frameshift_speech,\n",
    "        win_length=1024,\n",
    "        n_mel_channels=80,\n",
    "        sampling_rate=samplingFrequency,\n",
    "        mel_fmin=0,\n",
    "        mel_fmax=8000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ef706b",
   "metadata": {},
   "source": [
    "### **Load Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a83068a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "\n",
    "# Load Audio\n",
    "wavfile = 'data/stimuli/6min.wav'\n",
    "audio = AudioSegment.from_file(wavfile, format='wav')\n",
    "\n",
    "# Shift audio by 150 ms\n",
    "shifted_audio = audio._spawn(audio.raw_data, overrides={'frame_rate': audio.frame_rate, 'frame_width': audio.sample_width})\n",
    "shifted_audio = shifted_audio._spawn(shifted_audio.raw_data, overrides={'frame_rate': shifted_audio.frame_rate + int(22050*0.15)})\n",
    "shifted_audio = shifted_audio.set_frame_rate(audio.frame_rate)\n",
    "shifted_audio = shifted_audio.set_channels(audio.channels)\n",
    "\n",
    "# Export shifted audio as WAV file\n",
    "shifted_audio.export('shifted_audio.wav', format='wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22831591",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Audio\n",
    "wavfile = 'shifted_audio.wav'\n",
    "mel_data = WaveGlow_functions.get_mel(wavfile, stft)\n",
    "mel_data = np.fliplr(np.rot90(mel_data.data.numpy(), axes=(1, 0)))\n",
    "\n",
    "\n",
    "# Print out the duration of the original and shifted audio\n",
    "print(\"Original audio duration:\", audio.duration_seconds)\n",
    "print(\"Shifted audio duration:\", shifted_audio.duration_seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6866e010",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(mel_data, aspect='auto')\n",
    "plt.xlabel('Frame')\n",
    "plt.ylabel('Mel Bin')\n",
    "plt.title('Mel Spectrogram of the Audio')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa956309",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.transpose(mel_data), aspect='auto')\n",
    "plt.xlabel('Mel Bin')\n",
    "plt.ylabel('Frame')\n",
    "plt.title('Transposed Mel Spectrogram of the Audio')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2be4410",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Subjects\n",
    "bids_dir = 'data'\n",
    "subjects = mne_bids.get_entity_vals(bids_dir, 'subject')\n",
    "print(subjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1770f7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose subjects\n",
    "subject = '55'\n",
    "acquisition = 'clinical'\n",
    "task = 'film'\n",
    "datatype = 'ieeg'\n",
    "session = 'iemu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3022aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load subject's channels\n",
    "channels_path = mne_bids.BIDSPath(subject=subject,\n",
    "                                    session=session,\n",
    "                                    suffix='channels',\n",
    "                                    extension='tsv',\n",
    "                                    datatype=datatype,\n",
    "                                    task=task,\n",
    "                                    acquisition=acquisition,\n",
    "                                    root=bids_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989d5a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = pd.read_csv(str(channels_path.match()[0]), sep='\\t', header=0, index_col=None)\n",
    "#print(channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e2b3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set channel types\n",
    "data_path = mne_bids.BIDSPath(subject=subject,\n",
    "                                    session=session,\n",
    "                                    suffix='ieeg',\n",
    "                                    extension='vhdr',\n",
    "                                    datatype=datatype,\n",
    "                                    task=task,\n",
    "                                    acquisition=acquisition,\n",
    "                                    root=bids_dir)\n",
    "raw = mne.io.read_raw_brainvision(str(data_path.match()[0]), scale=1.0, preload=False, verbose=True)\n",
    "raw.set_channel_types({ch_name: str(x).lower()\n",
    "                if str(x).lower() in ['ecog', 'seeg', 'eeg'] else 'misc'\n",
    "                                for ch_name, x in zip(raw.ch_names, channels['type'].values)})\n",
    "raw.drop_channels([raw.ch_names[i] for i, j in enumerate(raw.get_channel_types()) if j == 'misc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501706bb",
   "metadata": {},
   "source": [
    "### Discard Bad Channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83882f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bad channels\n",
    "bad_channels = channels['name'][(channels['type'].isin(['ECOG', 'SEEG'])) & (channels['status'] == 'bad')].tolist()\n",
    "raw.info['bads'].extend([ch for ch in bad_channels])\n",
    "raw.drop_channels(raw.info['bads'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ad223c",
   "metadata": {},
   "source": [
    "### Load Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a51320d",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae79701",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_eeg_channels = int(raw.info['nchan']) # for subject 01\n",
    "print('n_eeg_channels', n_eeg_channels)\n",
    "# raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e11f21",
   "metadata": {},
   "source": [
    "### Apply notch filter to remove line noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a7e45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw.notch_filter(freqs=np.arange(50, 251, 50))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c1a751",
   "metadata": {},
   "source": [
    "raw.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1a92fe",
   "metadata": {},
   "source": [
    "### Apply common average reference to remove common noise and trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be08d03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CAR\n",
    "raw_car, _ = mne.set_eeg_reference(raw.copy(), 'average')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4bfcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = raw_car.copy().filter(60, 120).apply_hilbert(envelope=True).get_data()#.T\n",
    "print('raw_car.shape:', raw_car._data.shape, 'gamma shape: ', gamma.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad375f65",
   "metadata": {},
   "source": [
    "#Extract signal in gamma range, use Hilbert transform, but can also play around with wavelet decomposition options\n",
    "\n",
    "\n",
    "gamma = raw_car.copy().filter(60, 120).apply_hilbert(envelope=True).get_data().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b220ef9",
   "metadata": {},
   "source": [
    "### Read annotation with event markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c0237f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "custom_mapping = {'Stimulus/music': 2, 'Stimulus/speech': 1,\n",
    "                  'Stimulus/end task': 5}  # 'Stimulus/task end' in laan\n",
    "events, event_id = mne.events_from_annotations(raw_car, event_id=custom_mapping,\n",
    "                                                         use_rounding=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beda567e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting\n",
    "raw_car.plot(n_channels=30,scalings='auto', duration=3, start=29)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d88b433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume gamma is a 2D array\n",
    "np.savetxt(\"gamma.tsv\", gamma, delimiter=\"\\t\")\n",
    "\n",
    "n_melspec = 80\n",
    "#get EEG SR\n",
    "samplingFrequency_EEG=raw_car.info['sfreq']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58f7173",
   "metadata": {},
   "source": [
    "### Crop to keep only the segments while wathcing the stimuli ( 6.5 min long movie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f7bacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a copy taht we crop\n",
    "raw_car_cut = raw_car._data.copy()\n",
    "print(raw_car_cut.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c8a262",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('before cut: ', raw_car._data.shape, mel_data.shape)\n",
    "\n",
    "raw_car_cut = np.empty((n_eeg_channels,0))\n",
    "mel_data_cut = np.empty((0,n_melspec))\n",
    "\n",
    "\n",
    "# for i in range(6):\n",
    "for i in range(6):\n",
    "    start_time = events[2*i+1, 0] / raw_car.info['sfreq']\n",
    "    end_time = events[2*i+2, 0] / raw_car.info['sfreq']\n",
    "    start_idx, end_idx = raw_car.time_as_index([start_time, end_time])\n",
    "    print(i, 'iEEG index', start_idx, end_idx, end_idx-start_idx)\n",
    "    n_frames_per_sec = int(1 / frameshift_EEG)\n",
    "    print(i, 'melspec index', (2*i+1)*30*n_frames_per_sec, (2*i+2)*30*n_frames_per_sec, (2*i+2)*30*n_frames_per_sec-(2*i+1)*30*n_frames_per_sec)\n",
    "    # raw_car_cut1 = raw_car._data[:, start_idx:end_idx]\n",
    "    raw_car_cut1 = gamma[:, start_idx:end_idx]\n",
    "    raw_car_cut = np.append(raw_car_cut, raw_car_cut1, axis=1)\n",
    "    mel_data_cut1 = mel_data[(2*i+1)*30*n_frames_per_sec : (2*i+2)*30*n_frames_per_sec]\n",
    "    mel_data_cut = np.append(mel_data_cut, mel_data_cut1, axis=0)\n",
    "# raise\n",
    "mel_data = mel_data_cut\n",
    "\n",
    "print('after cut: ', raw_car_cut.shape, mel_data.shape)\n",
    "# raise\n",
    "#praat\n",
    "\n",
    "#get EEG SR\n",
    "samplingFrequency_EEG=raw_car.info['sfreq']\n",
    "\n",
    "# Calculate the length of the signal\n",
    "length = raw_car_cut.shape[1] / samplingFrequency_EEG \n",
    "print(\"The length of the EEG signal is\", length,\"s\")\n",
    "print(samplingFrequency_EEG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c8a1d3",
   "metadata": {},
   "source": [
    "### Extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60849991",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract HG features\n",
    "print('calculating Hilbert...', raw_car_cut.shape)\n",
    "# eeg_fft = np.empty((n_max_frames, n_freq_bands, n_eeg_channels * (2 * modelOrder_EEG + 1) ))\n",
    "# feat_Hilbert_1 = extractHG(raw_car_cut,samplingFrequency_EEG, windowLength=winL_EEG,frameshift=frameshift_EEG, bandpass_min=1, bandpass_max=200)\n",
    "feat_Hilbert_1 = extractHG(np.rot90(raw_car_cut),samplingFrequency_EEG, windowLength=winL_EEG,frameshift=frameshift_EEG, bandpass_min=1, bandpass_max=200)\n",
    "# feat_Hilbert_2 = extractHG(np.rot90(current_raw_eeg_data),samplingFrequency_EEG, windowLength=winL_EEG,frameshift=frameshift_EEG, bandpass_min=51, bandpass_max=100)\n",
    "# feat_Hilbert_3 = extractHG(np.rot90(current_raw_eeg_data),samplingFrequency_EEG, windowLength=winL_EEG,frameshift=frameshift_EEG, bandpass_min=101, bandpass_max=150)\n",
    "# feat_Hilbert_4 = extractHG(np.rot90(current_raw_eeg_data),samplingFrequency_EEG, windowLength=winL_EEG,frameshift=frameshift_EEG, bandpass_min=151, bandpass_max=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d54791b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stack features\n",
    "feat_Hilbert_1 = stackFeatures(feat_Hilbert_1,modelOrder=modelOrder_EEG,stepSize=stepSize_EEG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f189341e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.subplot(211)\n",
    "plt.imshow(np.rot90(feat_Hilbert_1), aspect='auto')\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.imshow(np.rot90(mel_data).T, aspect='auto')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "eeg = feat_Hilbert_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd726bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_len = np.min((len(eeg), len(mel_data)))\n",
    "eeg = eeg[0:min_len]\n",
    "mel_data = mel_data[0:min_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b80c971",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('mel & iEEG: ', mel_data.shape, feat_Hilbert_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231022c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index = np.arange(0, int(0.8 * eeg.shape[0]))\n",
    "test_index = np.arange(int(0.8 * eeg.shape[0]), eeg.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f977bb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-validation-test split\n",
    "eeg_train = eeg[0 : int(len(eeg) * 0.8)]\n",
    "eeg_valid = eeg[int(len(eeg) * 0.8) : int(len(eeg) * 0.9)]\n",
    "eeg_test =  eeg[int(len(eeg) * 0.9) : ]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95580bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "melspec_train = mel_data[0 : int(len(mel_data) * 0.8)]\n",
    "melspec_valid = mel_data[int(len(mel_data) * 0.8) : int(len(mel_data) * 0.9)]\n",
    "melspec_test =  mel_data[int(len(mel_data) * 0.9) : ]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45cbe30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale input to [0-1]\n",
    "eeg_scaler = MinMaxScaler()\n",
    "# eeg_scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "eeg_train_scaled = eeg_scaler.fit_transform(eeg_train)\n",
    "eeg_valid_scaled = eeg_scaler.transform(eeg_valid)\n",
    "eeg_test_scaled  = eeg_scaler.transform(eeg_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ea75cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale outpit mel-spectrogram data to zero mean, unit variances\n",
    "melspec_scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "melspec_train_scaled = melspec_scaler.fit_transform(melspec_train)\n",
    "melspec_valid_scaled = melspec_scaler.transform(melspec_valid)\n",
    "melspec_test_scaled  = melspec_scaler.transform(melspec_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4c26db",
   "metadata": {},
   "source": [
    "# **2D CNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea4e8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import MelFilterBank as mel\n",
    "import reconstructWave as rW\n",
    "import tensorflow\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, InputLayer, Dropout\n",
    "from keras import regularizers\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "import datetime\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import torch\n",
    "import sys\n",
    "import soundfile as sf\n",
    "import skimage.transform\n",
    "from tensorflow import keras\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7639158",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strided_app(a, L, S, verbose=None):  # Window len = L, Stride len/stepsize = S\n",
    "    shape = a.shape[1:]\n",
    "    nrows = ((a.shape[0] - L) // S) + 1\n",
    "    strides = a.strides\n",
    "    #print(shape, strides)\n",
    "    if verbose:\n",
    "        print(\"strides:\", strides)\n",
    "    return np.lib.stride_tricks.as_strided(a, shape=(nrows, L) + shape,\n",
    "                                           strides=(S * strides[0],) + strides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51476ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "method = '2D-CNN'\n",
    "result_path = os.path.join(os.getcwd(), f\"results_{method}\")\n",
    "winLength = 0.05\n",
    "frameshift = 0.01\n",
    "audiosr = 16000\n",
    "\n",
    "spectrogram = mel_data\n",
    "data = eeg\n",
    "pt=subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e94f0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d21e26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Create a train and test split from data, test is 20% of the data\n",
    "    train_index = np.arange(0, int(0.8 * data.shape[0]))\n",
    "    test_index = np.arange(int(0.8 * data.shape[0]), data.shape[0])\n",
    "\n",
    "    # Initialize an empty spectrogram to save the reconstruction to\n",
    "    rec_spec = np.zeros(spectrogram.shape)\n",
    "\n",
    "    # Z-Normalize with mean and std from the training data\n",
    "    mu = np.mean(data[train_index, :], axis=0)\n",
    "    std = np.std(data[train_index, :], axis=0)\n",
    "    trainData = (data[train_index, :] - mu) / std\n",
    "    testData = (data[test_index, :] - mu) / std\n",
    "\n",
    "    # Z-Normalize with mean and std from the training data -- output\n",
    "    mu = np.mean(spectrogram[train_index, :], axis=0)\n",
    "    std = np.std(spectrogram[train_index, :], axis=0)\n",
    "    trainSpectrogram = (spectrogram[train_index, :] - mu) / std\n",
    "    testSpectrogram = (spectrogram[test_index, :] - mu) / std\n",
    "\n",
    "    print('Input shape: ', trainData.shape)\n",
    "    print('Input shape: ', testData.shape)\n",
    "        \n",
    "    # Find the right shape for the input, as it should be 3D, like 1143 is 9*127\n",
    "    new_shape = int(trainData.shape[1] / 9)\n",
    "\n",
    "    # reshape input from 1143 to 9*127\n",
    "    trainData = trainData.reshape(-1, 9, new_shape)\n",
    "    testData = testData.reshape(-1, 9, new_shape)\n",
    "    print('Input shape: ', trainData.shape)\n",
    "\n",
    "    sts = 6\n",
    "    window_size = sts * 4 + 1\n",
    "    n_to_skip = np.floor(window_size // 2).astype(np.int64)\n",
    "\n",
    "    print('Input shape: ', trainData.shape)\n",
    "\n",
    "    #conversion to 3D blocks\n",
    "    trainData = strided_app(trainData, window_size, 1)\n",
    "    trainSpectrogram = trainSpectrogram[n_to_skip:(trainSpectrogram.shape[0] - n_to_skip)]\n",
    "\n",
    "    testData = strided_app(testData, window_size, 1)\n",
    "    testSpectrogram = testSpectrogram[n_to_skip:(testSpectrogram.shape[0] - n_to_skip)]\n",
    "\n",
    "    print('Input shape: ', trainData.shape)\n",
    "    print('Input/validation shape: ', testData.shape)\n",
    "    print('Output shape: ', trainSpectrogram.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d03b12",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "    model = Sequential()\n",
    "    model.add(InputLayer(input_shape=trainData.shape[1:]))\n",
    "    model.add(Conv2D(filters=40,\n",
    "                     kernel_size=(13, 13),\n",
    "                     strides=(sts, 2),\n",
    "                     activation=tensorflow.nn.swish,\n",
    "                     padding=\"same\",\n",
    "                     kernel_initializer=keras.initializers.he_uniform(seed=None),\n",
    "                     kernel_regularizer=regularizers.l1(0.00001),\n",
    "                     input_shape=trainData.shape[1:]))\n",
    "\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Conv2D(filters=400, kernel_size=(13, 13), strides=(2, 2), activation=tensorflow.nn.swish,\n",
    "                     padding=\"same\", kernel_initializer=keras.initializers.he_uniform(seed=None),\n",
    "                     kernel_regularizer=regularizers.l1(0.00001)))\n",
    "    model.add(Dropout(0.1))\n",
    "   # model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "   # model.add(Conv2D(filters=200, kernel_size=(13, 13), strides=(2, 2), activation=tensorflow.nn.swish,\n",
    "             #        padding=\"same\", kernel_initializer=keras.initializers.he_uniform(seed=None),\n",
    "             #        kernel_regularizer=regularizers.l1(0.00001)))\n",
    "  #  model.add(Dropout(0.1))\n",
    "    #model.add(Conv3D(filters=100, kernel_size=(1, 13, 13), strides=(1, 1, 1), activation=tensorflow.nn.swish,\n",
    "                     # padding=\"same\", kernel_initializer=keras.initializers.he_uniform(seed=None),\n",
    "                     # kernel_regularizer=regularizers.l1(0.00001)))\n",
    "    #model.add(Dropout(0.2))\n",
    "    #model.add(MaxPooling3D(pool_size=(1, 2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(\n",
    "        Dense(100000, activation=tensorflow.nn.swish, kernel_initializer=keras.initializers.he_uniform(seed=None),\n",
    "              bias_initializer=keras.initializers.he_uniform(seed=None),\n",
    "              kernel_regularizer=regularizers.l1(0.000005)))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(trainSpectrogram.shape[1], activation='linear'))\n",
    "\n",
    "    plot_model(model, to_file=f\"model_{method}.png\", show_shapes=True, show_layer_names=True)\n",
    "\n",
    "    model.compile(\n",
    "            loss='mean_squared_error',\n",
    "            metrics=['mean_squared_error'],\n",
    "            optimizer='adam')\n",
    "    earlystopper = EarlyStopping(monitor='val_mean_squared_error', min_delta=0.001, patience=3, verbose=1,\n",
    "                                 mode='auto')\n",
    "    lrr = ReduceLROnPlateau(monitor='val_mean_squared_error', patience=2, verbose=1, factor=0.5, min_lr=0.0001)\n",
    "\n",
    "    print(model.summary())\n",
    "\n",
    "    if not (os.path.isdir('models/')):\n",
    "        os.mkdir('models/')\n",
    "\n",
    "    # early stopping to avoid over-training\n",
    "    model_name = 'models/iEEG_to_melspec_2D-CNN_sp-' + pt\n",
    "\n",
    "    # csapot: temporarily disabled\n",
    "    checkp = ModelCheckpoint(\n",
    "        model_name +\n",
    "        '_weights_best.h5',\n",
    "        monitor='val_loss',\n",
    "        verbose=1,\n",
    "        save_best_only=True,\n",
    "        mode='min')\n",
    "\n",
    "    # Run training\n",
    "    history = model.fit(trainData, trainSpectrogram,\n",
    "                        epochs=100, batch_size=64, shuffle=False, verbose=1,\n",
    "                        callbacks=[earlystopper, checkp, lrr],\n",
    "                        validation_data=(testData, testSpectrogram),\n",
    "                        )\n",
    "\n",
    "\n",
    "    # load back best weights\n",
    "    model.load_weights(model_name + '_weights_best.h5')\n",
    "\n",
    "    rec_spec = model.predict(testData)\n",
    "\n",
    "    # inverse transform\n",
    "    # testSpectrogram=(spectrogram[test,:]-mu)/std\n",
    "    rec_spec = rec_spec * std + mu\n",
    "\n",
    "    print('start saving wav')\n",
    "\n",
    "    # Save reconstructed spectrogram\n",
    "    os.makedirs(os.path.join(result_path), exist_ok=True)\n",
    "    np.save(os.path.join(result_path, f'{pt}_predicted_spec.npy'), rec_spec)\n",
    "\n",
    "\n",
    "    # remove model file\n",
    "    os.remove(model_name + '_weights_best.h5')\n",
    "    del model\n",
    "    # Run garbage collection\n",
    "    gc.collect()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465474b2",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "best_val_mse = min(history.history['val_mean_squared_error'])\n",
    "\n",
    "fig, axs = plt.subplots(2, 1, figsize=(10, 5))\n",
    "\n",
    "axs[0].imshow(np.rot90(melspec_test_scaled[0:1000]), aspect='auto')\n",
    "axs[0].set_title('EEG Test Scaled')\n",
    "\n",
    "axs[1].imshow(np.rot90(rec_spec[0:1000]), aspect='auto')\n",
    "axs[1].set_title('2D-CNN')\n",
    "\n",
    "plt.subplots_adjust(hspace=0.4)\n",
    "plt.suptitle('2D-CNN results for patient ' + subject + ' ' + f'Best validation MSE: {best_val_mse:.4f}')\n",
    "plt.savefig(model_name + '_EEG_scaled_plots.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4735d1",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### Audio synth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dac5575",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Provide the actual mel spectrogram (melspec_predicted) and sampling frequency (samplingFrequency) from your FC-DNN output\n",
    "samplingFrequency = 512\n",
    "\n",
    "# Convert mel spectrogram to audio using Griffin-Lim algorithm\n",
    "audio = librosa.feature.inverse.mel_to_audio(melspec_predicted, sr=samplingFrequency)\n",
    "\n",
    "# Save the audio to a WAV file\n",
    "output_file = 'predicted_audio_2dcnn.wav'\n",
    "sf.write(output_file, audio, samplingFrequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824b0a8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
