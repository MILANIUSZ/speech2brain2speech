{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f572ea4c",
   "metadata": {},
   "source": [
    "# **2D-CNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ec5e1468",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mne\n",
    "import pandas as pd\n",
    "import mne_bids\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import MelFilterBank as mel\n",
    "import reconstructWave as rW\n",
    "import tensorflow\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, InputLayer, Dropout\n",
    "from keras import regularizers\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import torch\n",
    "import sys\n",
    "import soundfile as sf\n",
    "import skimage.transform\n",
    "from tensorflow import keras\n",
    "import gc\n",
    "import WaveGlow_functions\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import datetime\n",
    "from keras.callbacks import CSVLogger\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import argparse\n",
    "from subprocess import call, check_output, run\n",
    "import scipy.stats\n",
    "import scipy.io.wavfile\n",
    "import scipy.fftpack\n",
    "import scipy.io as sio\n",
    "import scipy\n",
    "import librosa\n",
    "import librosa.display\n",
    "import tensorflow as tf\n",
    "\n",
    "# Additional imports\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import mne\n",
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "import soundfile as sf\n",
    "\n",
    "# Set TensorFlow GPU memory growth\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "178db228",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-17 17:22:04.722396: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:26:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-05-17 17:22:04.722934: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:26:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-05-17 17:22:04.723356: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:26:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-05-17 17:22:04.723838: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:26:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-05-17 17:22:04.723872: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-05-17 17:22:04.724154: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:26:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-05-17 17:22:04.724231: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5577 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070, pci bus id: 0000:26:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "# do not use all GPU memory\n",
    "from tensorflow.compat.v1.keras.backend import set_session\n",
    "import tensorflow.compat.v1 as tf\n",
    "config = tf.ConfigProto()\n",
    "# config.gpu_options.per_process_gpu_memory_fraction = 0.3\n",
    "config.gpu_options.allow_growth = True \n",
    "sess = tf.Session(config=config)\n",
    "set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ba85a7",
   "metadata": {},
   "source": [
    "### **Helper functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a1c9bd06",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "#Small helper function to speed up the hilbert transform by extending the length of data to the next power of 2\n",
    "hilbert3 = lambda x: scipy.signal.hilbert(x, scipy.fftpack.next_fast_len(len(x)),axis=0)[:len(x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1160075d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def extractHG(data, sr, windowLength=0.05, frameshift=0.01, bandpass_min=70, bandpass_max=170):\n",
    "    \"\"\"\n",
    "    Window data and extract frequency-band envelope using the hilbert transform\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data: array (samples, channels)\n",
    "        EEG time series\n",
    "    sr: int\n",
    "        Sampling rate of the data\n",
    "    windowLength: float\n",
    "        Length of window (in seconds) in which spectrogram will be calculated\n",
    "    frameshift: float\n",
    "        Shift (in seconds) after which next window will be extracted\n",
    "    Returns\n",
    "    ----------\n",
    "    feat: array (windows, channels)\n",
    "        Frequency-band feature matrix\n",
    "    \"\"\"\n",
    "    #Linear detrend\n",
    "    data = scipy.signal.detrend(data,axis=0)\n",
    "    #Number of windows\n",
    "    numWindows = int(np.floor((data.shape[0]-windowLength*sr)/(frameshift*sr)))\n",
    "    #Filter High-Gamma Band\n",
    "    # sos = scipy.signal.iirfilter(4, [70/(sr/2),170/(sr/2)],btype='bandpass',output='sos')\n",
    "    sos = scipy.signal.iirfilter(4, [bandpass_min/(sr/2),bandpass_max/(sr/2)],btype='bandpass',output='sos')\n",
    "    data = scipy.signal.sosfiltfilt(sos,data,axis=0)\n",
    "    #Attenuate first harmonic of line noise\n",
    "    # sos = scipy.signal.iirfilter(4, [98/(sr/2),102/(sr/2)],btype='bandstop',output='sos')\n",
    "    # data = scipy.signal.sosfiltfilt(sos,data,axis=0)\n",
    "    #Attenuate second harmonic of line noise\n",
    "    # sos = scipy.signal.iirfilter(4, [148/(sr/2),152/(sr/2)],btype='bandstop',output='sos')\n",
    "    # data = scipy.signal.sosfiltfilt(sos,data,axis=0)\n",
    "    #Create feature space\n",
    "    data = np.abs(hilbert3(data))\n",
    "    feat = np.zeros((numWindows,data.shape[1]))\n",
    "    for win in range(numWindows):\n",
    "        start= int(np.floor((win*frameshift)*sr))\n",
    "        stop = int(np.floor(start+windowLength*sr))\n",
    "        feat[win,:] = np.mean(data[start:stop,:],axis=0)\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5740dbe7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def stackFeatures(features, modelOrder=4, stepSize=5):\n",
    "    \"\"\"\n",
    "    Add temporal context to each window by stacking neighboring feature vectors\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    features: array (windows, channels)\n",
    "        Feature time series\n",
    "    modelOrder: int\n",
    "        Number of temporal context to include prior to and after current window\n",
    "    stepSize: float\n",
    "        Number of temporal context to skip for each next context (to compensate for frameshift)\n",
    "    Returns\n",
    "    ----------\n",
    "    featStacked: array (windows, feat*(2*modelOrder+1))\n",
    "        Stacked feature matrix\n",
    "    \"\"\"\n",
    "    featStacked=np.zeros((features.shape[0]-(2*modelOrder*stepSize),(2*modelOrder+1)*features.shape[1]))\n",
    "    for fNum,i in enumerate(range(modelOrder*stepSize,features.shape[0]-modelOrder*stepSize)):\n",
    "        ef=features[i-modelOrder*stepSize:i+modelOrder*stepSize+1:stepSize,:]\n",
    "        featStacked[fNum,:]=ef.flatten() #Add 'F' if stacked the same as matlab\n",
    "    return featStacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7ec2377f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WaveGlow / Tacotron2 / STFT parameters for audio data\n",
    "# samplingFrequency = 16000\n",
    "samplingFrequency = 22050\n",
    "#samplingFrequency_EEG = 512 #sub 07\n",
    "winL_EEG = 0.05\n",
    "# frameshift_EEG = 0.01 # 10 ms\n",
    "frameshift_EEG = 0.01 # 10 ms\n",
    "frameshift_speech = 220 # 10ms\n",
    "# modelOrder_EEG = 1\n",
    "# modelOrder_EEG = 2\n",
    "modelOrder_EEG = 4\n",
    "# modelOrder_EEG = 10\n",
    "stepSize_EEG = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dac943e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/environment/notebooks/speech2brain2speech/WaveGlow_functions.py:136: FutureWarning: Pass size=1024 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  fft_window = pad_center(fft_window, filter_length)\n",
      "/environment/notebooks/speech2brain2speech/WaveGlow_functions.py:222: FutureWarning: Pass sr=22050, n_fft=1024, n_mels=80, fmin=0, fmax=8000 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  mel_basis = librosa_mel_fn(\n"
     ]
    }
   ],
   "source": [
    "stft = WaveGlow_functions.TacotronSTFT(\n",
    "        filter_length=1024,\n",
    "        hop_length=frameshift_speech,\n",
    "        win_length=1024,\n",
    "        n_mel_channels=80,\n",
    "        sampling_rate=samplingFrequency,\n",
    "        mel_fmin=0,\n",
    "        mel_fmax=8000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ef706b",
   "metadata": {},
   "source": [
    "### **Load Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9a83068a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_io.BufferedRandom name='shifted_audio.wav'>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydub import AudioSegment\n",
    "\n",
    "# Load Audio\n",
    "wavfile = 'data/stimuli/6min.wav'\n",
    "audio = AudioSegment.from_file(wavfile, format='wav')\n",
    "\n",
    "# Shift audio by 150 ms\n",
    "shifted_audio = audio._spawn(audio.raw_data, overrides={'frame_rate': audio.frame_rate, 'frame_width': audio.sample_width})\n",
    "shifted_audio = shifted_audio._spawn(shifted_audio.raw_data, overrides={'frame_rate': shifted_audio.frame_rate + int(22050*0.15)})\n",
    "shifted_audio = shifted_audio.set_frame_rate(audio.frame_rate)\n",
    "shifted_audio = shifted_audio.set_channels(audio.channels)\n",
    "\n",
    "# Export shifted audio as WAV file\n",
    "shifted_audio.export('shifted_audio.wav', format='wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "22831591",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Load Audio\u001b[39;00m\n\u001b[1;32m      2\u001b[0m wavfile \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshifted_audio.wav\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 3\u001b[0m mel_data \u001b[38;5;241m=\u001b[39m \u001b[43mWaveGlow_functions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_mel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwavfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstft\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m mel_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfliplr(np\u001b[38;5;241m.\u001b[39mrot90(mel_data\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mnumpy(), axes\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m)))\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Print out the duration of the original and shifted audio\u001b[39;00m\n",
      "File \u001b[0;32m/environment/notebooks/speech2brain2speech/WaveGlow_functions.py:261\u001b[0m, in \u001b[0;36mget_mel\u001b[0;34m(filename, stft)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_mel\u001b[39m(filename,stft):\n\u001b[0;32m--> 261\u001b[0m             audio, sampling_rate \u001b[38;5;241m=\u001b[39m \u001b[43mload_wav_to_torch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    262\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m sampling_rate \u001b[38;5;241m!=\u001b[39m stft\u001b[38;5;241m.\u001b[39msampling_rate:\n\u001b[1;32m    263\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m SR doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt match target \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m SR\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    264\u001b[0m                     sampling_rate, stft\u001b[38;5;241m.\u001b[39msampling_rate))\n",
      "File \u001b[0;32m/environment/notebooks/speech2brain2speech/WaveGlow_functions.py:256\u001b[0m, in \u001b[0;36mload_wav_to_torch\u001b[0;34m(full_path)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_wav_to_torch\u001b[39m(full_path):\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;66;03m#sampling_rate, data = read(full_path)\u001b[39;00m\n\u001b[0;32m--> 256\u001b[0m     data,sampling_rate \u001b[38;5;241m=\u001b[39m \u001b[43mlibrosa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43msr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m22050\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mmono\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;66;03m# data,sampling_rate = librosa.load(full_path,sr=16000,mono=True)\u001b[39;00m\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mFloatTensor(data\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)), sampling_rate\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/librosa/util/decorators.py:88\u001b[0m, in \u001b[0;36mdeprecate_positional_args.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m extra_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(all_args)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m extra_args \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# extra_args > 0\u001b[39;00m\n\u001b[1;32m     91\u001b[0m args_msg \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(name, arg)\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(kwonly_args[:extra_args], args[\u001b[38;5;241m-\u001b[39mextra_args:])\n\u001b[1;32m     94\u001b[0m ]\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/librosa/core/audio.py:179\u001b[0m, in \u001b[0;36mload\u001b[0;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[1;32m    176\u001b[0m     y \u001b[38;5;241m=\u001b[39m to_mono(y)\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mresample\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morig_sr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msr_native\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_sr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mres_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mres_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    182\u001b[0m     sr \u001b[38;5;241m=\u001b[39m sr_native\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/librosa/util/decorators.py:88\u001b[0m, in \u001b[0;36mdeprecate_positional_args.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m extra_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(all_args)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m extra_args \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# extra_args > 0\u001b[39;00m\n\u001b[1;32m     91\u001b[0m args_msg \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(name, arg)\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(kwonly_args[:extra_args], args[\u001b[38;5;241m-\u001b[39mextra_args:])\n\u001b[1;32m     94\u001b[0m ]\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/librosa/core/audio.py:647\u001b[0m, in \u001b[0;36mresample\u001b[0;34m(y, orig_sr, target_sr, res_type, fix, scale, **kwargs)\u001b[0m\n\u001b[1;32m    645\u001b[0m     y_hat \u001b[38;5;241m=\u001b[39m soxr\u001b[38;5;241m.\u001b[39mresample(y\u001b[38;5;241m.\u001b[39mT, orig_sr, target_sr, quality\u001b[38;5;241m=\u001b[39mres_type)\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m    646\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 647\u001b[0m     y_hat \u001b[38;5;241m=\u001b[39m \u001b[43mresampy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresample\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morig_sr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_sr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mres_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fix:\n\u001b[1;32m    650\u001b[0m     y_hat \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mfix_length(y_hat, size\u001b[38;5;241m=\u001b[39mn_samples, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/resampy/core.py:168\u001b[0m, in \u001b[0;36mresample\u001b[0;34m(x, sr_orig, sr_new, axis, filter, parallel, **kwargs)\u001b[0m\n\u001b[1;32m    158\u001b[0m         resample_f_s(\n\u001b[1;32m    159\u001b[0m             x\u001b[38;5;241m.\u001b[39mswapaxes(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, axis),\n\u001b[1;32m    160\u001b[0m             t_out,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    165\u001b[0m             y\u001b[38;5;241m.\u001b[39mswapaxes(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, axis),\n\u001b[1;32m    166\u001b[0m         )\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 168\u001b[0m     \u001b[43mresample_f_s\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mswapaxes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_out\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43minterp_win\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43minterp_delta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprecision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mswapaxes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/numba/np/ufunc/gufunc.py:192\u001b[0m, in \u001b[0;36mGUFunc.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd(sig)\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_ufunc()\n\u001b[0;32m--> 192\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mufunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Load Audio\n",
    "wavfile = 'shifted_audio.wav'\n",
    "mel_data = WaveGlow_functions.get_mel(wavfile, stft)\n",
    "mel_data = np.fliplr(np.rot90(mel_data.data.numpy(), axes=(1, 0)))\n",
    "\n",
    "\n",
    "# Print out the duration of the original and shifted audio\n",
    "print(\"Original audio duration:\", audio.duration_seconds)\n",
    "print(\"Shifted audio duration:\", shifted_audio.duration_seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6866e010",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(mel_data, aspect='auto')\n",
    "plt.xlabel('Frame')\n",
    "plt.ylabel('Mel Bin')\n",
    "plt.title('Mel Spectrogram of the Audio')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa956309",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.transpose(mel_data), aspect='auto')\n",
    "plt.xlabel('Mel Bin')\n",
    "plt.ylabel('Frame')\n",
    "plt.title('Transposed Mel Spectrogram of the Audio')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2be4410",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Subjects\n",
    "bids_dir = 'data'\n",
    "subjects = mne_bids.get_entity_vals(bids_dir, 'subject')\n",
    "print(subjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1770f7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose subjects\n",
    "subject = '55'\n",
    "acquisition = 'clinical'\n",
    "task = 'film'\n",
    "datatype = 'ieeg'\n",
    "session = 'iemu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3022aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load subject's channels\n",
    "channels_path = mne_bids.BIDSPath(subject=subject,\n",
    "                                    session=session,\n",
    "                                    suffix='channels',\n",
    "                                    extension='tsv',\n",
    "                                    datatype=datatype,\n",
    "                                    task=task,\n",
    "                                    acquisition=acquisition,\n",
    "                                    root=bids_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989d5a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = pd.read_csv(str(channels_path.match()[0]), sep='\\t', header=0, index_col=None)\n",
    "#print(channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e2b3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set channel types\n",
    "data_path = mne_bids.BIDSPath(subject=subject,\n",
    "                                    session=session,\n",
    "                                    suffix='ieeg',\n",
    "                                    extension='vhdr',\n",
    "                                    datatype=datatype,\n",
    "                                    task=task,\n",
    "                                    acquisition=acquisition,\n",
    "                                    root=bids_dir)\n",
    "raw = mne.io.read_raw_brainvision(str(data_path.match()[0]), scale=1.0, preload=False, verbose=True)\n",
    "raw.set_channel_types({ch_name: str(x).lower()\n",
    "                if str(x).lower() in ['ecog', 'seeg', 'eeg'] else 'misc'\n",
    "                                for ch_name, x in zip(raw.ch_names, channels['type'].values)})\n",
    "raw.drop_channels([raw.ch_names[i] for i, j in enumerate(raw.get_channel_types()) if j == 'misc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501706bb",
   "metadata": {},
   "source": [
    "### Discard Bad Channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83882f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bad channels\n",
    "bad_channels = channels['name'][(channels['type'].isin(['ECOG', 'SEEG'])) & (channels['status'] == 'bad')].tolist()\n",
    "raw.info['bads'].extend([ch for ch in bad_channels])\n",
    "raw.drop_channels(raw.info['bads'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ad223c",
   "metadata": {},
   "source": [
    "### Load Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a51320d",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae79701",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_eeg_channels = int(raw.info['nchan']) # for subject 01\n",
    "print('n_eeg_channels', n_eeg_channels)\n",
    "# raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e11f21",
   "metadata": {},
   "source": [
    "### Apply notch filter to remove line noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a7e45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw.notch_filter(freqs=np.arange(50, 251, 50))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c1a751",
   "metadata": {},
   "source": [
    "raw.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1a92fe",
   "metadata": {},
   "source": [
    "### Apply common average reference to remove common noise and trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be08d03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CAR\n",
    "raw_car, _ = mne.set_eeg_reference(raw.copy(), 'average')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4bfcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = raw_car.copy().filter(60, 120).apply_hilbert(envelope=True).get_data()#.T\n",
    "print('raw_car.shape:', raw_car._data.shape, 'gamma shape: ', gamma.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad375f65",
   "metadata": {},
   "source": [
    "#Extract signal in gamma range, use Hilbert transform, but can also play around with wavelet decomposition options\n",
    "\n",
    "\n",
    "gamma = raw_car.copy().filter(60, 120).apply_hilbert(envelope=True).get_data().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b220ef9",
   "metadata": {},
   "source": [
    "### Read annotation with event markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c0237f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "custom_mapping = {'Stimulus/music': 2, 'Stimulus/speech': 1,\n",
    "                  'Stimulus/end task': 5}  # 'Stimulus/task end' in laan\n",
    "events, event_id = mne.events_from_annotations(raw_car, event_id=custom_mapping,\n",
    "                                                         use_rounding=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beda567e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting\n",
    "raw_car.plot(n_channels=30,scalings='auto', duration=3, start=29)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d88b433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume gamma is a 2D array\n",
    "np.savetxt(\"gamma.tsv\", gamma, delimiter=\"\\t\")\n",
    "\n",
    "n_melspec = 80\n",
    "#get EEG SR\n",
    "samplingFrequency_EEG=raw_car.info['sfreq']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58f7173",
   "metadata": {},
   "source": [
    "### Crop to keep only the segments while wathcing the stimuli ( 6.5 min long movie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f7bacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a copy taht we crop\n",
    "raw_car_cut = raw_car._data.copy()\n",
    "print(raw_car_cut.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c8a262",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('before cut: ', raw_car._data.shape, mel_data.shape)\n",
    "\n",
    "raw_car_cut = np.empty((n_eeg_channels,0))\n",
    "mel_data_cut = np.empty((0,n_melspec))\n",
    "\n",
    "\n",
    "# for i in range(6):\n",
    "for i in range(6):\n",
    "    start_time = events[2*i+1, 0] / raw_car.info['sfreq']\n",
    "    end_time = events[2*i+2, 0] / raw_car.info['sfreq']\n",
    "    start_idx, end_idx = raw_car.time_as_index([start_time, end_time])\n",
    "    print(i, 'iEEG index', start_idx, end_idx, end_idx-start_idx)\n",
    "    n_frames_per_sec = int(1 / frameshift_EEG)\n",
    "    print(i, 'melspec index', (2*i+1)*30*n_frames_per_sec, (2*i+2)*30*n_frames_per_sec, (2*i+2)*30*n_frames_per_sec-(2*i+1)*30*n_frames_per_sec)\n",
    "    # raw_car_cut1 = raw_car._data[:, start_idx:end_idx]\n",
    "    raw_car_cut1 = gamma[:, start_idx:end_idx]\n",
    "    raw_car_cut = np.append(raw_car_cut, raw_car_cut1, axis=1)\n",
    "    mel_data_cut1 = mel_data[(2*i+1)*30*n_frames_per_sec : (2*i+2)*30*n_frames_per_sec]\n",
    "    mel_data_cut = np.append(mel_data_cut, mel_data_cut1, axis=0)\n",
    "# raise\n",
    "mel_data = mel_data_cut\n",
    "\n",
    "print('after cut: ', raw_car_cut.shape, mel_data.shape)\n",
    "# raise\n",
    "#praat\n",
    "\n",
    "#get EEG SR\n",
    "samplingFrequency_EEG=raw_car.info['sfreq']\n",
    "\n",
    "# Calculate the length of the signal\n",
    "length = raw_car_cut.shape[1] / samplingFrequency_EEG \n",
    "print(\"The length of the EEG signal is\", length,\"s\")\n",
    "print(samplingFrequency_EEG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c8a1d3",
   "metadata": {},
   "source": [
    "### Extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60849991",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract HG features\n",
    "print('calculating Hilbert...', raw_car_cut.shape)\n",
    "# eeg_fft = np.empty((n_max_frames, n_freq_bands, n_eeg_channels * (2 * modelOrder_EEG + 1) ))\n",
    "# feat_Hilbert_1 = extractHG(raw_car_cut,samplingFrequency_EEG, windowLength=winL_EEG,frameshift=frameshift_EEG, bandpass_min=1, bandpass_max=200)\n",
    "feat_Hilbert_1 = extractHG(np.rot90(raw_car_cut),samplingFrequency_EEG, windowLength=winL_EEG,frameshift=frameshift_EEG, bandpass_min=1, bandpass_max=200)\n",
    "# feat_Hilbert_2 = extractHG(np.rot90(current_raw_eeg_data),samplingFrequency_EEG, windowLength=winL_EEG,frameshift=frameshift_EEG, bandpass_min=51, bandpass_max=100)\n",
    "# feat_Hilbert_3 = extractHG(np.rot90(current_raw_eeg_data),samplingFrequency_EEG, windowLength=winL_EEG,frameshift=frameshift_EEG, bandpass_min=101, bandpass_max=150)\n",
    "# feat_Hilbert_4 = extractHG(np.rot90(current_raw_eeg_data),samplingFrequency_EEG, windowLength=winL_EEG,frameshift=frameshift_EEG, bandpass_min=151, bandpass_max=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d54791b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stack features\n",
    "feat_Hilbert_1 = stackFeatures(feat_Hilbert_1,modelOrder=modelOrder_EEG,stepSize=stepSize_EEG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f189341e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.subplot(211)\n",
    "plt.imshow(np.rot90(feat_Hilbert_1), aspect='auto')\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.imshow(np.rot90(mel_data).T, aspect='auto')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "eeg = feat_Hilbert_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd726bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_len = np.min((len(eeg), len(mel_data)))\n",
    "eeg = eeg[0:min_len]\n",
    "mel_data = mel_data[0:min_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b80c971",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('mel & iEEG: ', mel_data.shape, feat_Hilbert_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231022c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index = np.arange(0, int(0.8 * eeg.shape[0]))\n",
    "test_index = np.arange(int(0.8 * eeg.shape[0]), eeg.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f977bb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-validation-test split\n",
    "eeg_train = eeg[0 : int(len(eeg) * 0.8)]\n",
    "eeg_valid = eeg[int(len(eeg) * 0.8) : int(len(eeg) * 0.9)]\n",
    "eeg_test =  eeg[int(len(eeg) * 0.9) : ]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95580bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "melspec_train = mel_data[0 : int(len(mel_data) * 0.8)]\n",
    "melspec_valid = mel_data[int(len(mel_data) * 0.8) : int(len(mel_data) * 0.9)]\n",
    "melspec_test =  mel_data[int(len(mel_data) * 0.9) : ]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45cbe30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale input to [0-1]\n",
    "eeg_scaler = MinMaxScaler()\n",
    "# eeg_scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "eeg_train_scaled = eeg_scaler.fit_transform(eeg_train)\n",
    "eeg_valid_scaled = eeg_scaler.transform(eeg_valid)\n",
    "eeg_test_scaled  = eeg_scaler.transform(eeg_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ea75cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale outpit mel-spectrogram data to zero mean, unit variances\n",
    "melspec_scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "melspec_train_scaled = melspec_scaler.fit_transform(melspec_train)\n",
    "melspec_valid_scaled = melspec_scaler.transform(melspec_valid)\n",
    "melspec_test_scaled  = melspec_scaler.transform(melspec_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4c26db",
   "metadata": {},
   "source": [
    "# **2D CNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea4e8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import MelFilterBank as mel\n",
    "import reconstructWave as rW\n",
    "import tensorflow\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, InputLayer, Dropout\n",
    "from keras import regularizers\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "import datetime\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import torch\n",
    "import sys\n",
    "import soundfile as sf\n",
    "import skimage.transform\n",
    "from tensorflow import keras\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7639158",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strided_app(a, L, S, verbose=None):  # Window len = L, Stride len/stepsize = S\n",
    "    shape = a.shape[1:]\n",
    "    nrows = ((a.shape[0] - L) // S) + 1\n",
    "    strides = a.strides\n",
    "    #print(shape, strides)\n",
    "    if verbose:\n",
    "        print(\"strides:\", strides)\n",
    "    return np.lib.stride_tricks.as_strided(a, shape=(nrows, L) + shape,\n",
    "                                           strides=(S * strides[0],) + strides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51476ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "method = '2D-CNN'\n",
    "result_path = os.path.join(os.getcwd(), f\"results_{method}\")\n",
    "winLength = 0.05\n",
    "frameshift = 0.01\n",
    "audiosr = 16000\n",
    "\n",
    "spectrogram = mel_data\n",
    "data = eeg\n",
    "pt=subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e94f0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d21e26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Create a train and test split from data, test is 20% of the data\n",
    "    train_index = np.arange(0, int(0.8 * data.shape[0]))\n",
    "    test_index = np.arange(int(0.8 * data.shape[0]), data.shape[0])\n",
    "\n",
    "    # Initialize an empty spectrogram to save the reconstruction to\n",
    "    rec_spec = np.zeros(spectrogram.shape)\n",
    "\n",
    "    # Z-Normalize with mean and std from the training data\n",
    "    mu = np.mean(data[train_index, :], axis=0)\n",
    "    std = np.std(data[train_index, :], axis=0)\n",
    "    trainData = (data[train_index, :] - mu) / std\n",
    "    testData = (data[test_index, :] - mu) / std\n",
    "\n",
    "    # Z-Normalize with mean and std from the training data -- output\n",
    "    mu = np.mean(spectrogram[train_index, :], axis=0)\n",
    "    std = np.std(spectrogram[train_index, :], axis=0)\n",
    "    trainSpectrogram = (spectrogram[train_index, :] - mu) / std\n",
    "    testSpectrogram = (spectrogram[test_index, :] - mu) / std\n",
    "\n",
    "    print('Input shape: ', trainData.shape)\n",
    "    print('Input shape: ', testData.shape)\n",
    "        \n",
    "    # Find the right shape for the input, as it should be 3D, like 1143 is 9*127\n",
    "    new_shape = int(trainData.shape[1] / 9)\n",
    "\n",
    "    # reshape input from 1143 to 9*127\n",
    "    trainData = trainData.reshape(-1, 9, new_shape)\n",
    "    testData = testData.reshape(-1, 9, new_shape)\n",
    "    print('Input shape: ', trainData.shape)\n",
    "\n",
    "    sts = 6\n",
    "    window_size = sts * 4 + 1\n",
    "    n_to_skip = np.floor(window_size // 2).astype(np.int64)\n",
    "\n",
    "    print('Input shape: ', trainData.shape)\n",
    "\n",
    "    #conversion to 3D blocks\n",
    "    trainData = strided_app(trainData, window_size, 1)\n",
    "    trainSpectrogram = trainSpectrogram[n_to_skip:(trainSpectrogram.shape[0] - n_to_skip)]\n",
    "\n",
    "    testData = strided_app(testData, window_size, 1)\n",
    "    testSpectrogram = testSpectrogram[n_to_skip:(testSpectrogram.shape[0] - n_to_skip)]\n",
    "\n",
    "    print('Input shape: ', trainData.shape)\n",
    "    print('Input/validation shape: ', testData.shape)\n",
    "    print('Output shape: ', trainSpectrogram.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d03b12",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "    model = Sequential()\n",
    "    model.add(InputLayer(input_shape=trainData.shape[1:]))\n",
    "    model.add(Conv2D(filters=40,\n",
    "                     kernel_size=(13, 13),\n",
    "                     strides=(sts, 2),\n",
    "                     activation=tensorflow.nn.swish,\n",
    "                     padding=\"same\",\n",
    "                     kernel_initializer=keras.initializers.he_uniform(seed=None),\n",
    "                     kernel_regularizer=regularizers.l1(0.00001),\n",
    "                     input_shape=trainData.shape[1:]))\n",
    "\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Conv2D(filters=400, kernel_size=(13, 13), strides=(2, 2), activation=tensorflow.nn.swish,\n",
    "                     padding=\"same\", kernel_initializer=keras.initializers.he_uniform(seed=None),\n",
    "                     kernel_regularizer=regularizers.l1(0.00001)))\n",
    "    model.add(Dropout(0.1))\n",
    "   # model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "   # model.add(Conv2D(filters=200, kernel_size=(13, 13), strides=(2, 2), activation=tensorflow.nn.swish,\n",
    "             #        padding=\"same\", kernel_initializer=keras.initializers.he_uniform(seed=None),\n",
    "             #        kernel_regularizer=regularizers.l1(0.00001)))\n",
    "  #  model.add(Dropout(0.1))\n",
    "    #model.add(Conv3D(filters=100, kernel_size=(1, 13, 13), strides=(1, 1, 1), activation=tensorflow.nn.swish,\n",
    "                     # padding=\"same\", kernel_initializer=keras.initializers.he_uniform(seed=None),\n",
    "                     # kernel_regularizer=regularizers.l1(0.00001)))\n",
    "    #model.add(Dropout(0.2))\n",
    "    #model.add(MaxPooling3D(pool_size=(1, 2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(\n",
    "        Dense(100000, activation=tensorflow.nn.swish, kernel_initializer=keras.initializers.he_uniform(seed=None),\n",
    "              bias_initializer=keras.initializers.he_uniform(seed=None),\n",
    "              kernel_regularizer=regularizers.l1(0.000005)))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(trainSpectrogram.shape[1], activation='linear'))\n",
    "\n",
    "    plot_model(model, to_file=f\"model_{method}.png\", show_shapes=True, show_layer_names=True)\n",
    "\n",
    "    model.compile(\n",
    "            loss='mean_squared_error',\n",
    "            metrics=['mean_squared_error'],\n",
    "            optimizer='adam')\n",
    "    earlystopper = EarlyStopping(monitor='val_mean_squared_error', min_delta=0.001, patience=3, verbose=1,\n",
    "                                 mode='auto')\n",
    "    lrr = ReduceLROnPlateau(monitor='val_mean_squared_error', patience=2, verbose=1, factor=0.5, min_lr=0.0001)\n",
    "\n",
    "    print(model.summary())\n",
    "\n",
    "    if not (os.path.isdir('models/')):\n",
    "        os.mkdir('models/')\n",
    "\n",
    "    # early stopping to avoid over-training\n",
    "    model_name = 'models/iEEG_to_melspec_2D-CNN_sp-' + pt\n",
    "\n",
    "    # csapot: temporarily disabled\n",
    "    checkp = ModelCheckpoint(\n",
    "        model_name +\n",
    "        '_weights_best.h5',\n",
    "        monitor='val_loss',\n",
    "        verbose=1,\n",
    "        save_best_only=True,\n",
    "        mode='min')\n",
    "\n",
    "    # Run training\n",
    "    history = model.fit(trainData, trainSpectrogram,\n",
    "                        epochs=100, batch_size=64, shuffle=False, verbose=1,\n",
    "                        callbacks=[earlystopper, checkp, lrr],\n",
    "                        validation_data=(testData, testSpectrogram),\n",
    "                        )\n",
    "\n",
    "\n",
    "    # load back best weights\n",
    "    model.load_weights(model_name + '_weights_best.h5')\n",
    "\n",
    "    rec_spec = model.predict(testData)\n",
    "\n",
    "    # inverse transform\n",
    "    # testSpectrogram=(spectrogram[test,:]-mu)/std\n",
    "    rec_spec = rec_spec * std + mu\n",
    "\n",
    "    print('start saving wav')\n",
    "\n",
    "    # Save reconstructed spectrogram\n",
    "    os.makedirs(os.path.join(result_path), exist_ok=True)\n",
    "    np.save(os.path.join(result_path, f'{pt}_predicted_spec.npy'), rec_spec)\n",
    "\n",
    "\n",
    "    # remove model file\n",
    "    os.remove(model_name + '_weights_best.h5')\n",
    "    del model\n",
    "    # Run garbage collection\n",
    "    gc.collect()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465474b2",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "best_val_mse = min(history.history['val_mean_squared_error'])\n",
    "\n",
    "fig, axs = plt.subplots(2, 1, figsize=(10, 5))\n",
    "\n",
    "axs[0].imshow(np.rot90(melspec_test_scaled[0:1000]), aspect='auto')\n",
    "axs[0].set_title('EEG Test Scaled')\n",
    "\n",
    "axs[1].imshow(np.rot90(rec_spec[0:1000]), aspect='auto')\n",
    "axs[1].set_title('2D-CNN')\n",
    "\n",
    "plt.subplots_adjust(hspace=0.4)\n",
    "plt.suptitle('2D-CNN results for patient ' + subject + ' ' + f'Best validation MSE: {best_val_mse:.4f}')\n",
    "plt.savefig(model_name + '_EEG_scaled_plots.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301cdd92",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dac5575",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824b0a8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
