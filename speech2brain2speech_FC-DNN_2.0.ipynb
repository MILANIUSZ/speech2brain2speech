{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f572ea4c",
   "metadata": {},
   "source": [
    "# **FC-DNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec5e1468",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmne\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/numpy/__init__.py:160\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m linalg\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fft\n\u001b[0;32m--> 160\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m polynomial\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m random\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ctypeslib\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/numpy/polynomial/__init__.py:120\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegendre\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Legendre\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhermite\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Hermite\n\u001b[0;32m--> 120\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhermite_e\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HermiteE\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlaguerre\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Laguerre\n\u001b[1;32m    123\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset_default_printstyle\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpolynomial\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPolynomial\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlaguerre\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLaguerre\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    131\u001b[0m ]\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:991\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:975\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:671\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:844\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:939\u001b[0m, in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1038\u001b[0m, in \u001b[0;36mget_data\u001b[0;34m(self, path)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import mne\n",
    "import pandas as pd\n",
    "import mne_bids\n",
    "import matplotlib.pyplot as plt\n",
    "import soundfile as sf\n",
    "import os\n",
    "import pickle\n",
    "from datetime import datetime, timedelta\n",
    "from pydub import AudioSegment\n",
    "from Extra import WaveGlow_functions\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import EarlyStopping, CSVLogger, ModelCheckpoint\n",
    "import tensorflow as tf\n",
    "import scipy.signal\n",
    "import scipy.io as sio\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6acbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not use all GPU memory\n",
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "tf.compat.v1.keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ba85a7",
   "metadata": {},
   "source": [
    "### **Helper functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c9bd06",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "#Small helper function to speed up the hilbert transform by extending the length of data to the next power of 2\n",
    "hilbert3 = lambda x: scipy.signal.hilbert(\n",
    "    x, scipy.fftpack.next_fast_len(len(x)), axis=0)[:len(x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1160075d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def extractHG(data,\n",
    "              sr,\n",
    "              windowLength=0.05,\n",
    "              frameshift=0.01,\n",
    "              bandpass_min=1,\n",
    "              bandpass_max=70):\n",
    "    \"\"\"\n",
    "    Window data and extract frequency-band envelope using the hilbert transform\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data: array (samples, channels)\n",
    "        EEG time series\n",
    "    sr: int\n",
    "        Sampling rate of the data\n",
    "    windowLength: float\n",
    "        Length of window (in seconds) in which spectrogram will be calculated\n",
    "    frameshift: float\n",
    "        Shift (in seconds) after which next window will be extracted\n",
    "    Returns\n",
    "    ----------\n",
    "    feat: array (windows, channels)\n",
    "        Frequency-band feature matrix\n",
    "    \"\"\"\n",
    "    #Linear detrend\n",
    "    data = scipy.signal.detrend(data, axis=0)\n",
    "    #Number of windows\n",
    "    numWindows = int(\n",
    "        np.floor((data.shape[0] - windowLength * sr) / (frameshift * sr)))\n",
    "    #Filter High-Gamma Band\n",
    "    # sos = scipy.signal.iirfilter(4, [70/(sr/2),170/(sr/2)],btype='bandpass',output='sos')\n",
    "    sos = scipy.signal.iirfilter(\n",
    "        4, [bandpass_min / (sr / 2), bandpass_max / (sr / 2)],\n",
    "        btype='bandpass',\n",
    "        output='sos')\n",
    "    data = scipy.signal.sosfiltfilt(sos, data, axis=0)\n",
    "    #Attenuate first harmonic of line noise\n",
    "    # sos = scipy.signal.iirfilter(4, [98/(sr/2),102/(sr/2)],btype='bandstop',output='sos')\n",
    "    # data = scipy.signal.sosfiltfilt(sos,data,axis=0)\n",
    "    #Attenuate second harmonic of line noise\n",
    "    # sos = scipy.signal.iirfilter(4, [148/(sr/2),152/(sr/2)],btype='bandstop',output='sos')\n",
    "    # data = scipy.signal.sosfiltfilt(sos,data,axis=0)\n",
    "    #Create feature space\n",
    "    data = np.abs(hilbert3(data))\n",
    "    feat = np.zeros((numWindows, data.shape[1]))\n",
    "    for win in range(numWindows):\n",
    "        start = int(np.floor((win * frameshift) * sr))\n",
    "        stop = int(np.floor(start + windowLength * sr))\n",
    "        feat[win, :] = np.mean(data[start:stop, :], axis=0)\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5740dbe7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def stackFeatures(features, modelOrder=4, stepSize=5):\n",
    "    \"\"\"\n",
    "    Add temporal context to each window by stacking neighboring feature vectors\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    features: array (windows, channels)\n",
    "        Feature time series\n",
    "    modelOrder: int\n",
    "        Number of temporal context to include prior to and after current window\n",
    "    stepSize: float\n",
    "        Number of temporal context to skip for each next context (to compensate for frameshift)\n",
    "    Returns\n",
    "    ----------\n",
    "    featStacked: array (windows, feat*(2*modelOrder+1))\n",
    "        Stacked feature matrix\n",
    "    \"\"\"\n",
    "    featStacked = np.zeros((features.shape[0] - (2 * modelOrder * stepSize),\n",
    "                            (2 * modelOrder + 1) * features.shape[1]))\n",
    "    for fNum, i in enumerate(\n",
    "            range(modelOrder * stepSize,\n",
    "                  features.shape[0] - modelOrder * stepSize)):\n",
    "        ef = features[i - modelOrder * stepSize:i + modelOrder * stepSize +\n",
    "                      1:stepSize, :]\n",
    "        featStacked[\n",
    "            fNum, :] = ef.flatten()  #Add 'F' if stacked the same as matlab\n",
    "    return featStacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec2377f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WaveGlow / Tacotron2 / STFT parameters for audio data\n",
    "# samplingFrequency = 16000\n",
    "#samplingFrequency = 22050\n",
    "samplingFrequency = 22050\n",
    "#samplingFrequency_EEG = 512 #sub 07\n",
    "winL_EEG = 0.05\n",
    "# frameshift_EEG = 0.01 # 10 ms\n",
    "frameshift_EEG = 0.01  # 10 ms\n",
    "frameshift_speech = 220  # 10ms\n",
    "# modelOrder_EEG = 1\n",
    "# modelOrder_EEG = 2\n",
    "modelOrder_EEG = 4\n",
    "# modelOrder_EEG = 10\n",
    "stepSize_EEG = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac943e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "stft = WaveGlow_functions.TacotronSTFT(filter_length=1024,\n",
    "                                       hop_length=frameshift_speech,\n",
    "                                       win_length=1024,\n",
    "                                       n_mel_channels=80,\n",
    "                                       sampling_rate=samplingFrequency,\n",
    "                                       mel_fmin=0,\n",
    "                                       mel_fmax=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acafe0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import soundfile as sf\n",
    "\n",
    "# Load the original audio file\n",
    "y, sr = librosa.load('data/stimuli/6min.wav', sr=None)  # Load with original sampling rate\n",
    "\n",
    "print(\"Sampling rate before:\", sr, \"Hz\")\n",
    "# Resample to 22050 Hz\n",
    "y_22050 = librosa.resample(y, orig_sr=sr, target_sr=22050)\n",
    "\n",
    "# Save the resampled audio to a new file\n",
    "sf.write('resampled_audio_22050Hz.wav', y_22050, 22050)\n",
    "\n",
    "# Use the resampled file in your processing pipeline\n",
    "wavfile = 'resampled_audio_22050Hz.wav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a806610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the resampled audio file\n",
    "y, sr = librosa.load('resampled_audio_22050Hz.wav', sr=22050)\n",
    "\n",
    "# Shift the audio by 150 ms\n",
    "shift_ms = 150  # milliseconds\n",
    "shift_samples = int(sr * (shift_ms / 1000.0))  # Convert milliseconds to samples\n",
    "shifted_audio = np.pad(y, (shift_samples, 0), mode='constant', constant_values=(0, 0))\n",
    "\n",
    "# Save the shifted audio to a new file\n",
    "sf.write('shifted_audio_22050Hz.wav', shifted_audio, sr)\n",
    "\n",
    "# Now you can compute the mel-spectrogram of the shifted audio\n",
    "wavfile = 'shifted_audio_22050Hz.wav'\n",
    "mel_data = WaveGlow_functions.get_mel(wavfile, stft)\n",
    "mel_data_nonrot = mel_data\n",
    "mel_data = np.fliplr(np.rot90(mel_data.data.numpy(), axes=(1, 0)))\n",
    "\n",
    "\n",
    "# Print shapes and durations\n",
    "\n",
    "print(\"Original audio duration (seconds):\", len(y) / sr)\n",
    "print(\"Shifted audio duration (seconds):\", len(shifted_audio) / sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9de4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "\n",
    "# Plot the waveform\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(np.linspace(0, len(y) / sr, len(y)), y)\n",
    "plt.title('Waveform of Audio File')\n",
    "plt.xlabel('Time (seconds)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.show()\n",
    "\n",
    "# Plot the mel-spectrogram before rotation and flipping\n",
    "plt.figure(figsize=(10, 4))\n",
    "librosa.display.specshow(librosa.power_to_db(mel_data_nonrot, ref=np.max), sr=sr, hop_length=512, y_axis='mel', x_axis='time')\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.title('Mel-Spectrogram Before Transformation')\n",
    "plt.show()\n",
    "\n",
    "# Plot the mel-spectrogram after rotation and flipping\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.imshow(mel_data, aspect='auto', origin='lower')\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.title('Mel-Spectrogram After Transformation')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Mel Bin')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ef706b",
   "metadata": {},
   "source": [
    "### **Load Audio**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0339f1",
   "metadata": {},
   "source": [
    "# Shift audio by 150 ms\n",
    "shifted_audio = audio._spawn(audio.raw_data,\n",
    "                             overrides={\n",
    "                                 'frame_rate': audio.frame_rate,\n",
    "                                 'frame_width': audio.sample_width\n",
    "                             })\n",
    "shifted_audio = shifted_audio._spawn(\n",
    "    shifted_audio.raw_data,\n",
    "    overrides={'frame_rate': shifted_audio.frame_rate + int(22050 * 0.15)})\n",
    "shifted_audio = shifted_audio.set_frame_rate(audio.frame_rate)\n",
    "shifted_audio = shifted_audio.set_channels(audio.channels)\n",
    "\n",
    "# Export shifted audio as WAV file\n",
    "shifted_audio.export('shifted_audio.wav', format='wav')\n",
    "\n",
    "#Load Audio\n",
    "wavfile = 'shifted_audio.wav'\n",
    "mel_data = WaveGlow_functions.get_mel(wavfile, stft)\n",
    "og_mel_data = mel_data\n",
    "mel_data = np.fliplr(np.rot90(mel_data.data.numpy(), axes=(1, 0)))\n",
    "og_mel_data_flipped= mel_data\n",
    "#mel_data = mel_data.data.numpy()\n",
    "\n",
    "\n",
    "print(mel_data.shape)\n",
    "# Print out the duration of the original and shifted audio\n",
    "print(\"Original audio duration:\", audio.duration_seconds)\n",
    "print(\"Shifted audio duration:\", shifted_audio.duration_seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2be4410",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Subjects\n",
    "bids_dir = 'data'\n",
    "subjects = mne_bids.get_entity_vals(bids_dir, 'subject')\n",
    "#Choose subjects\n",
    "subject = '13'\n",
    "acquisition = 'clinical'\n",
    "task = 'film'\n",
    "datatype = 'ieeg'\n",
    "session = 'iemu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3022aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load subject's channels\n",
    "channels_path = mne_bids.BIDSPath(subject=subject,\n",
    "                                  session=session,\n",
    "                                  suffix='channels',\n",
    "                                  extension='tsv',\n",
    "                                  datatype=datatype,\n",
    "                                  task=task,\n",
    "                                  acquisition=acquisition,\n",
    "                                  root=bids_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989d5a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = pd.read_csv(str(channels_path.match()[0]),\n",
    "                       sep='\\t',\n",
    "                       header=0,\n",
    "                       index_col=None)\n",
    "#print(channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e2b3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set channel types\n",
    "data_path = mne_bids.BIDSPath(subject=subject,\n",
    "                              session=session,\n",
    "                              suffix='ieeg',\n",
    "                              extension='vhdr',\n",
    "                              datatype=datatype,\n",
    "                              task=task,\n",
    "                              acquisition=acquisition,\n",
    "                              root=bids_dir)\n",
    "raw = mne.io.read_raw_brainvision(str(data_path.match()[0]),\n",
    "                                  scale=1.0,\n",
    "                                  preload=False,\n",
    "                                  verbose=True)\n",
    "raw.set_channel_types({\n",
    "    ch_name:\n",
    "    str(x).lower() if str(x).lower() in ['ecog', 'seeg', 'eeg'] else 'misc'\n",
    "    for ch_name, x in zip(raw.ch_names, channels['type'].values)\n",
    "})\n",
    "raw.drop_channels([\n",
    "    raw.ch_names[i] for i, j in enumerate(raw.get_channel_types())\n",
    "    if j == 'misc'\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a36615",
   "metadata": {},
   "source": [
    "print(raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501706bb",
   "metadata": {},
   "source": [
    "### Discard Bad Channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83882f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bad channels\n",
    "bad_channels = channels['name'][(channels['type'].isin(['ECOG', 'SEEG']))\n",
    "                                & (channels['status'] == 'bad')].tolist()\n",
    "raw.info['bads'].extend([ch for ch in bad_channels])\n",
    "raw.drop_channels(raw.info['bads'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ad223c",
   "metadata": {},
   "source": [
    "### Load Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a51320d",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae79701",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_eeg_channels = int(raw.info['nchan'])  # for subject 01\n",
    "print('n_eeg_channels', n_eeg_channels)\n",
    "# raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e11f21",
   "metadata": {},
   "source": [
    "### Apply notch filter to remove line noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a7e45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw.notch_filter(freqs=np.arange(50, 251, 50))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1a92fe",
   "metadata": {},
   "source": [
    "### Apply common average reference to remove common noise and trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be08d03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#CAR\n",
    "raw_car, _ = mne.set_eeg_reference(raw.copy(), 'average')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b220ef9",
   "metadata": {},
   "source": [
    "### Read annotation with event markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c0237f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "custom_mapping = {\n",
    "    'Stimulus/music': 2,\n",
    "    'Stimulus/speech': 1,\n",
    "    'Stimulus/end task': 5\n",
    "}  # 'Stimulus/task end' in laan\n",
    "events, event_id = mne.events_from_annotations(raw_car,\n",
    "                                               event_id=custom_mapping,\n",
    "                                               use_rounding=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beda567e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting\n",
    "raw_car.plot(n_channels=30, scalings='auto', duration=20, start=109)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7fa713",
   "metadata": {},
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a5638f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get EEG SR\n",
    "n_melspec = 80\n",
    "samplingFrequency_EEG = raw_car.info['sfreq']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f7bacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "eeg=raw_car.get_data()\n",
    "\n",
    "\n",
    "#create a copy taht we crop\n",
    "raw_car_cut = raw_car.get_data()\n",
    "print(raw_car_cut.shape)\n",
    "#(n_channels, n_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c8a262",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('before cut: ', raw_car._data.shape, mel_data.shape)\n",
    "\n",
    "raw_car_cut = np.empty((n_eeg_channels, 0))\n",
    "mel_data_cut = np.empty((0, n_melspec))\n",
    "\n",
    "# for i in range(6):\n",
    "for i in range(6):\n",
    "    start_time = events[2 * i + 1, 0] / raw_car.info['sfreq']\n",
    "    end_time = events[2 * i + 2, 0] / raw_car.info['sfreq']\n",
    "    start_idx, end_idx = raw_car.time_as_index([start_time, end_time])\n",
    "    print(i, 'iEEG index', start_idx, end_idx, end_idx - start_idx)\n",
    "    n_frames_per_sec = int(1 / frameshift_EEG)\n",
    "    print(i, 'melspec index', (2 * i + 1) * 30 * n_frames_per_sec,\n",
    "          (2 * i + 2) * 30 * n_frames_per_sec,\n",
    "          (2 * i + 2) * 30 * n_frames_per_sec -\n",
    "          (2 * i + 1) * 30 * n_frames_per_sec)\n",
    "    # raw_car_cut1 = raw_car._data[:, start_idx:end_idx]\n",
    "    raw_car_cut1 = eeg[:, start_idx:end_idx]\n",
    "    raw_car_cut = np.append(raw_car_cut, raw_car_cut1, axis=1)\n",
    "    mel_data_cut1 = mel_data[(2 * i + 1) * 30 * n_frames_per_sec:(2 * i + 2) *\n",
    "                             30 * n_frames_per_sec]\n",
    "    mel_data_cut = np.append(mel_data_cut, mel_data_cut1, axis=0)\n",
    "# raise\n",
    "mel_data = mel_data_cut\n",
    "\n",
    "print('after cut: ', raw_car_cut.shape, mel_data.shape)\n",
    "# raise\n",
    "#praat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b5b054",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get EEG SR\n",
    "samplingFrequency_EEG = raw_car.info['sfreq']\n",
    "\n",
    "# Calculate the length of the signal\n",
    "length = raw_car_cut.shape[1] / samplingFrequency_EEG\n",
    "\n",
    "print(\"The length of the EEG signal is\", length, \"s\")\n",
    "print(samplingFrequency_EEG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a49b6d",
   "metadata": {},
   "source": [
    "### Prepare Data for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c8a1d3",
   "metadata": {},
   "source": [
    "### Extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60849991",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract HG features\n",
    "print('calculating Hilbert...', raw_car_cut.shape)\n",
    "# eeg_fft = np.empty((n_max_frames, n_freq_bands, n_eeg_channels * (2 * modelOrder_EEG + 1) ))\n",
    "# feat_Hilbert_1 = extractHG(raw_car_cut,samplingFrequency_EEG, windowLength=winL_EEG,frameshift=frameshift_EEG, bandpass_min=1, bandpass_max=200)\n",
    "feat_Hilbert_1 = extractHG(np.rot90(raw_car_cut),\n",
    "                           samplingFrequency_EEG,\n",
    "                           windowLength=winL_EEG,\n",
    "                           frameshift=frameshift_EEG,\n",
    "                           bandpass_min=1,\n",
    "                           bandpass_max=70)\n",
    "# feat_Hilbert_2 = extractHG(np.rot90(current_raw_eeg_data),samplingFrequency_EEG, windowLength=winL_EEG,frameshift=frameshift_EEG, bandpass_min=51, bandpass_max=100)\n",
    "# feat_Hilbert_3 = extractHG(np.rot90(current_raw_eeg_data),samplingFrequency_EEG, windowLength=winL_EEG,frameshift=frameshift_EEG, bandpass_min=101, bandpass_max=150)\n",
    "# feat_Hilbert_4 = extractHG(np.rot90(current_raw_eeg_data),samplingFrequency_EEG, windowLength=winL_EEG,frameshift=frameshift_EEG, bandpass_min=151, bandpass_max=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d54791b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stack features\n",
    "feat_Hilbert_1 = stackFeatures(feat_Hilbert_1,\n",
    "                               modelOrder=modelOrder_EEG,\n",
    "                               stepSize=stepSize_EEG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f189341e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(211)\n",
    "plt.imshow(np.rot90(feat_Hilbert_1), aspect='auto')\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.imshow(np.rot90(mel_data), aspect='auto')\n",
    "plt.show()\n",
    "\n",
    "eeg = feat_Hilbert_1\n",
    "print('mel & iEEG: ', mel_data.shape, feat_Hilbert_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd726bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_len = np.min((len(eeg), len(mel_data)))\n",
    "eeg = eeg[0:min_len]\n",
    "mel_data = mel_data[0:min_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b80c971",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('mel & iEEG: ', mel_data.shape, eeg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231022c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index = np.arange(0, int(0.8 * eeg.shape[0]))\n",
    "test_index = np.arange(int(0.8 * eeg.shape[0]), eeg.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f977bb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-validation-test split\n",
    "eeg_train = eeg[0:int(len(eeg) * 0.8)]\n",
    "eeg_valid = eeg[int(len(eeg) * 0.8):int(len(eeg) * 0.9)]\n",
    "eeg_test = eeg[int(len(eeg) * 0.9):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95580bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "melspec_train = mel_data[0:int(len(mel_data) * 0.8)]\n",
    "melspec_valid = mel_data[int(len(mel_data) * 0.8):int(len(mel_data) * 0.9)]\n",
    "melspec_test = mel_data[int(len(mel_data) * 0.9):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45cbe30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale input to [0-1]\n",
    "eeg_scaler = MinMaxScaler()\n",
    "# eeg_scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "eeg_train_scaled = eeg_scaler.fit_transform(eeg_train)\n",
    "eeg_valid_scaled = eeg_scaler.transform(eeg_valid)\n",
    "eeg_test_scaled = eeg_scaler.transform(eeg_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ea75cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale outpit mel-spectrogram data to zero mean, unit variances\n",
    "melspec_scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "melspec_train_scaled = melspec_scaler.fit_transform(melspec_train)\n",
    "melspec_valid_scaled = melspec_scaler.transform(melspec_valid)\n",
    "melspec_test_scaled = melspec_scaler.transform(melspec_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b42b4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming mel_data is your original mel-spectrogram and melspec_test_scaled is the scaled version\n",
    "# Also assuming both are NumPy arrays\n",
    "\n",
    "# Plot original mel-spectrogram\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.imshow(mel_data.T, aspect='auto', origin='lower')\n",
    "plt.title('Original Mel-Spectrogram')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Mel Bin')\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot scaled mel-spectrogram\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.imshow(melspec_test_scaled.T, aspect='auto', origin='lower')\n",
    "plt.title('Scaled Mel-Spectrogram')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Mel Bin')\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c2cc3f",
   "metadata": {},
   "source": [
    "### FC-DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb69759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 hidden layers, with 1000 neuron on each layer\n",
    "model = Sequential()\n",
    "model.add(\n",
    "    Dense(1000,\n",
    "          input_dim=eeg_train_scaled.shape[1],\n",
    "          kernel_initializer='normal',\n",
    "          activation='relu'))\n",
    "model.add(\n",
    "    Dense(1000,\n",
    "          input_dim=eeg_train_scaled.shape[1],\n",
    "          kernel_initializer='normal',\n",
    "          activation='relu'))\n",
    "model.add(\n",
    "    Dense(1000,\n",
    "          input_dim=eeg_train_scaled.shape[1],\n",
    "          kernel_initializer='normal',\n",
    "          activation='relu'))\n",
    "model.add(Dense(80, kernel_initializer='normal', activation='linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2453c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model\n",
    "model.compile(loss='mean_squared_error',\n",
    "              metrics=['mean_squared_error'],\n",
    "              optimizer='adam')\n",
    "earlystopper = EarlyStopping(monitor='val_mean_squared_error',\n",
    "                             min_delta=0.0001,\n",
    "                             patience=5,\n",
    "                             verbose=1,\n",
    "                             mode='auto')\n",
    "\n",
    "if not (os.path.isdir('models_iEEG_to_melspec/')):\n",
    "    os.mkdir('models_iEEG_to_melspec/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec442183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# early stopping to avoid over-training\n",
    "# csv logger\n",
    "current_date = '{date:%Y-%m-%d_%H-%M-%S}'.format(date=datetime.now())\n",
    "print(current_date)\n",
    "# n_eeg_channels * (2 * modelOrder_EEG + 1)\n",
    "model_name = 'models_iEEG_to_melspec/iEEG-Hilbert_to_melspec_DNN_modelOrder-' + str(\n",
    "    modelOrder_EEG).zfill(\n",
    "        2) + '_freqBands-1_' + '_sub' + subject + '_' + current_date\n",
    "logger = CSVLogger(model_name + '.csv', append=True, separator=';')\n",
    "checkp = ModelCheckpoint(model_name + '_weights_best.h5',\n",
    "                         monitor='val_loss',\n",
    "                         verbose=1,\n",
    "                         save_best_only=True,\n",
    "                         mode='min')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea32df2",
   "metadata": {},
   "source": [
    "serialize scalers to pickle\n",
    "pickle.dump(eeg_scaler, open(model_name + '_eeg_scaler.sav', 'wb'))\n",
    "pickle.dump(melspec_scaler, open(model_name + '_melspec_scaler.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685666f3",
   "metadata": {},
   "source": [
    "### Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f957197",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    eeg_train_scaled,\n",
    "    melspec_train_scaled,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    verbose=1,\n",
    "    callbacks=[earlystopper, logger, checkp],\n",
    "    validation_split=0.9,\n",
    "    validation_data=(eeg_valid_scaled, melspec_valid_scaled),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925c663e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "model_json = model.to_json()\n",
    "with open(model_name + '_model.json', \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "# here the training of the DNN is finished\n",
    "# load back best weights\n",
    "model.load_weights(model_name + '_weights_best.h5')\n",
    "# remove model file\n",
    "# os.remove(model_name + '_weights_best.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b48ac7",
   "metadata": {},
   "source": [
    "### Visualize predicted melspectogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01e70a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# melspec_predicted = model.predict(eeg_test_scaled[0:500])\n",
    "melspec_predicted = model.predict(eeg_test_scaled[0:1000])\n",
    "# melspec_predicted = melspec_predicted[0:500]\n",
    "# test_melspec = test_melspec[]\n",
    "\n",
    "#best stuff\n",
    "\n",
    "best_val_mse = min(history.history['val_mean_squared_error'])\n",
    "print(f'Best validation MSE: {best_val_mse:.4f}')\n",
    "\n",
    "min_train_loss = min(history.history['loss'])\n",
    "print(f'Minimum training loss: {min_train_loss:.4f}')\n",
    "\n",
    "fig, axs = plt.subplots(3, 1, figsize=(10, 10))\n",
    "axs[0].imshow(np.rot90(eeg_test_scaled[0:1000]), aspect='auto')\n",
    "axs[0].set_title('EEG Test Scaled')\n",
    "axs[1].imshow(np.rot90(melspec_test_scaled[0:1000]), aspect='auto')\n",
    "axs[1].set_title('Mel-Spectrogram Test Scaled')\n",
    "axs[2].imshow(np.rot90(melspec_predicted[0:1000]), aspect='auto')\n",
    "axs[2].set_title('Mel-Spectrogram Predicted')\n",
    "plt.suptitle(\n",
    "    f'FC-DNN results (scaled) for subject {subject}\\nBest validation MSE: {best_val_mse:.4f}, Minimum training loss: {min_train_loss:.4f}'\n",
    ")\n",
    "plt.subplots_adjust(hspace=0.4)\n",
    "\n",
    "plt.savefig(model_name + '_EEG_scaled_plots.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9039e1e3",
   "metadata": {},
   "source": [
    "### Audio synth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a4107f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Provide the actual mel spectrogram (melspec_predicted) and sampling frequency (samplingFrequency) from your FC-DNN output\n",
    "samplingFrequency = 16000\n",
    "\n",
    "# Convert mel spectrogram to audio using Griffin-Lim algorithm\n",
    "audio = librosa.feature.inverse.mel_to_audio(melspec_predicted,\n",
    "                                             sr=samplingFrequency)\n",
    "\n",
    "# Save the audio to a WAV file\n",
    "output_file = subject + ' predicted_audio_fc-DNN.wav'\n",
    "sf.write(output_file, audio, samplingFrequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1e7adb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dad5cfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2892e517",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530033f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e6af55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "1caeedbc",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
